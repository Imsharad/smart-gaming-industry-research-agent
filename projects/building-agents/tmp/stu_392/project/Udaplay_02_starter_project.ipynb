{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdd0bcb",
   "metadata": {},
   "source": [
    "# [STARTER] Udaplay Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325b035",
   "metadata": {},
   "source": [
    "## Part 02 - Agent\n",
    "\n",
    "In this part of the project, you'll use your VectorDB to be part of your Agent as a tool.\n",
    "\n",
    "You're building UdaPlay, an AI Research Agent for the video game industry. The agent will:\n",
    "1. Answer questions using internal knowledge (RAG)\n",
    "2. Search the web when needed\n",
    "3. Maintain conversation state\n",
    "4. Return structured outputs\n",
    "5. Store useful information for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42de90",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only needed for Udacity workspace\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Check if 'pysqlite3' is available before importing\n",
    "if importlib.util.find_spec(\"pysqlite3\") is not None:\n",
    "    import pysqlite3\n",
    "    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import json\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from dotenv import load_dotenv\n",
    "from tavily import TavilyClient\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "from lib.agents import Agent\n",
    "from lib.llm import LLM\n",
    "from lib.messages import UserMessage, SystemMessage, ToolMessage, AIMessage\n",
    "from lib.tooling import Tool, tool\n",
    "from lib.parsers import PydanticOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e465d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "CHROMA_OPENAI_API_KEY = os.getenv(\"CHROMA_OPENAI_API_KEY\") or OPENAI_API_KEY\n",
    "\n",
    "# Verify API keys are loaded\n",
    "assert OPENAI_API_KEY is not None, \"OPENAI_API_KEY not found in environment variables\"\n",
    "assert TAVILY_API_KEY is not None, \"TAVILY_API_KEY not found in environment variables\"\n",
    "\n",
    "print(\"✓ Environment variables loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de4729",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab2dac",
   "metadata": {},
   "source": [
    "Build at least 3 tools:\n",
    "- retrieve_game: To search the vector DB\n",
    "- evaluate_retrieval: To assess the retrieval performance\n",
    "- game_web_search: If no good, search the web\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f14cd",
   "metadata": {},
   "source": [
    "#### Retrieve Game Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChromaDB client and collection\n",
    "chroma_client = chromadb.PersistentClient(path=\"chromadb\")\n",
    "embedding_fn = embedding_functions.OpenAIEmbeddingFunction(api_key=CHROMA_OPENAI_API_KEY)\n",
    "collection = chroma_client.get_collection(name=\"udaplay\", embedding_function=embedding_fn)\n",
    "\n",
    "@tool\n",
    "def retrieve_game(query: str) -> str:\n",
    "    \"\"\"\n",
    "    Semantic search: Finds most relevant results in the vector DB.\n",
    "    \n",
    "    Args:\n",
    "        query: a question about game industry.\n",
    "    \n",
    "    Returns:\n",
    "        Formatted string with game information. Each result contains:\n",
    "        - Platform: like Game Boy, Playstation 5, Xbox 360...\n",
    "        - Name: Name of the Game\n",
    "        - YearOfRelease: Year when that game was released for that platform\n",
    "        - Description: Additional details about the game\n",
    "    \"\"\"\n",
    "    # Query the vector database\n",
    "    results = collection.query(\n",
    "        query_texts=[query],\n",
    "        n_results=5,\n",
    "        include=['documents', 'metadatas', 'distances']\n",
    "    )\n",
    "    \n",
    "    if not results['documents'][0]:\n",
    "        return \"No games found matching the query.\"\n",
    "    \n",
    "    # Format results\n",
    "    formatted_results = []\n",
    "    for i, (doc, metadata, distance) in enumerate(zip(\n",
    "        results['documents'][0],\n",
    "        results['metadatas'][0],\n",
    "        results['distances'][0]\n",
    "    ), 1):\n",
    "        result_str = f\"Result {i} (relevance score: {1-distance:.4f}):\\n\"\n",
    "        result_str += f\"  Name: {metadata['Name']}\\n\"\n",
    "        result_str += f\"  Platform: {metadata['Platform']}\\n\"\n",
    "        result_str += f\"  Year of Release: {metadata['YearOfRelease']}\\n\"\n",
    "        result_str += f\"  Genre: {metadata.get('Genre', 'N/A')}\\n\"\n",
    "        result_str += f\"  Publisher: {metadata.get('Publisher', 'N/A')}\\n\"\n",
    "        result_str += f\"  Description: {metadata['Description']}\\n\"\n",
    "        formatted_results.append(result_str)\n",
    "    \n",
    "    return \"\\n\".join(formatted_results)\n",
    "\n",
    "print(\"✓ retrieve_game tool created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910dc945",
   "metadata": {},
   "source": [
    "#### Evaluate Retrieval Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define EvaluationReport model for structured output\n",
    "class EvaluationReport(BaseModel):\n",
    "    \"\"\"Evaluation result for retrieved documents\"\"\"\n",
    "    useful: bool = Field(description=\"Whether the documents are useful to answer the question\")\n",
    "    description: str = Field(description=\"Detailed explanation about the evaluation result\")\n",
    "\n",
    "@tool\n",
    "def evaluate_retrieval(question: str, retrieved_docs: str) -> str:\n",
    "    \"\"\"\n",
    "    Based on the user's question and on the list of retrieved documents, \n",
    "    it will analyze the usability of the documents to respond to that question.\n",
    "    \n",
    "    Args:\n",
    "        question: original question from user\n",
    "        retrieved_docs: retrieved documents most similar to the user query in the Vector Database\n",
    "    \n",
    "    Returns:\n",
    "        Evaluation result with 'useful' boolean and 'description' explanation.\n",
    "        The result includes:\n",
    "        - useful: whether the documents are useful to answer the question\n",
    "        - description: description about the evaluation result\n",
    "    \"\"\"\n",
    "    # Use LLM as judge to evaluate retrieval quality\n",
    "    judge_llm = LLM(model=\"gpt-4o-mini\", temperature=0.0)\n",
    "    \n",
    "    evaluation_prompt = f\"\"\"Your task is to evaluate if the documents are enough to respond to the query.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Retrieved Documents:\n",
    "{retrieved_docs}\n",
    "\n",
    "Evaluate whether these documents contain sufficient information to answer the question accurately.\n",
    "Give a detailed explanation, so it's possible to take an action to accept it or not.\n",
    "\n",
    "Respond with a JSON object containing:\n",
    "- useful: true if the documents are sufficient, false otherwise\n",
    "- description: a detailed explanation of your evaluation\n",
    "\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Get structured output using response_format\n",
    "        response = judge_llm.invoke(\n",
    "            evaluation_prompt,\n",
    "            response_format=EvaluationReport\n",
    "        )\n",
    "        \n",
    "        # Parse the response - the content should be JSON\n",
    "        parser = PydanticOutputParser(model_class=EvaluationReport)\n",
    "        evaluation = parser.parse(response)\n",
    "        \n",
    "        # Format result\n",
    "        result = {\n",
    "            \"useful\": evaluation.useful,\n",
    "            \"description\": evaluation.description\n",
    "        }\n",
    "    except Exception as e:\n",
    "        # Fallback: try to parse as JSON directly\n",
    "        try:\n",
    "            response = judge_llm.invoke(evaluation_prompt)\n",
    "            content = response.content\n",
    "            # Try to extract JSON from the content\n",
    "            if \"{\" in content and \"}\" in content:\n",
    "                start = content.find(\"{\")\n",
    "                end = content.rfind(\"}\") + 1\n",
    "                json_str = content[start:end]\n",
    "                result = json.loads(json_str)\n",
    "            else:\n",
    "                # Fallback evaluation\n",
    "                result = {\n",
    "                    \"useful\": False,\n",
    "                    \"description\": f\"Could not parse evaluation. Error: {str(e)}\"\n",
    "                }\n",
    "        except:\n",
    "            result = {\n",
    "                \"useful\": False,\n",
    "                \"description\": f\"Evaluation failed: {str(e)}\"\n",
    "            }\n",
    "    \n",
    "    return json.dumps(result, indent=2)\n",
    "\n",
    "print(\"✓ evaluate_retrieval tool created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7935a26",
   "metadata": {},
   "source": [
    "#### Game Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad698aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Tavily client\n",
    "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n",
    "\n",
    "@tool\n",
    "def game_web_search(question: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs web search to find information about video games when internal \n",
    "    knowledge is insufficient.\n",
    "    \n",
    "    Args:\n",
    "        question: a question about game industry.\n",
    "    \n",
    "    Returns:\n",
    "        Search results with relevant information and source citations.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Perform web search using Tavily\n",
    "        response = tavily_client.search(\n",
    "            query=question,\n",
    "            search_depth=\"advanced\",\n",
    "            max_results=5\n",
    "        )\n",
    "        \n",
    "        # Format results with citations\n",
    "        formatted_results = []\n",
    "        formatted_results.append(f\"Web search results for: {question}\\n\")\n",
    "        \n",
    "        if 'results' in response:\n",
    "            for i, result in enumerate(response['results'], 1):\n",
    "                formatted_results.append(f\"Result {i}:\")\n",
    "                formatted_results.append(f\"  Title: {result.get('title', 'N/A')}\")\n",
    "                formatted_results.append(f\"  URL: {result.get('url', 'N/A')}\")\n",
    "                formatted_results.append(f\"  Content: {result.get('content', 'N/A')[:500]}...\")\n",
    "                formatted_results.append(\"\")\n",
    "        \n",
    "        # Include answer if available\n",
    "        if 'answer' in response and response['answer']:\n",
    "            formatted_results.append(f\"\\nSummary Answer: {response['answer']}\")\n",
    "        \n",
    "        return \"\\n\".join(formatted_results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error performing web search: {str(e)}\"\n",
    "\n",
    "print(\"✓ game_web_search tool created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df844b3b",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c56281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with system instructions and tools\n",
    "system_instructions = \"\"\"You are UdaPlay, an AI Research Agent specialized in answering questions about video games.\n",
    "\n",
    "Your workflow:\n",
    "1. First, try to answer using internal knowledge by calling retrieve_game with the user's question\n",
    "2. Evaluate the retrieved results using evaluate_retrieval to assess if they're sufficient\n",
    "3. If the evaluation indicates the results are not useful or insufficient, use game_web_search to find additional information\n",
    "4. Combine information from all sources to provide a comprehensive answer\n",
    "5. Always cite your sources (internal database or web sources)\n",
    "\n",
    "Guidelines:\n",
    "- Be accurate and factual\n",
    "- Cite sources in your responses\n",
    "- If information is not available, clearly state that\n",
    "- Provide structured, readable answers\n",
    "- When using web search results, include URLs as citations\n",
    "\n",
    "Tools available:\n",
    "- retrieve_game: Search internal game database\n",
    "- evaluate_retrieval: Assess if retrieved documents are sufficient\n",
    "- game_web_search: Search the web for additional information\n",
    "\"\"\"\n",
    "\n",
    "# Create tools list\n",
    "tools = [\n",
    "    Tool.from_func(retrieve_game),\n",
    "    Tool.from_func(evaluate_retrieval),\n",
    "    Tool.from_func(game_web_search)\n",
    "]\n",
    "\n",
    "# Create agent\n",
    "agent = Agent(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    instructions=system_instructions,\n",
    "    tools=tools,\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(\"✓ Agent created with all tools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec23893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Query 1: \"When was Pokémon Gold and Silver released?\"\n",
    "query1 = \"When was Pokémon Gold and Silver released?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Query 1: {query1}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "run1 = agent.invoke(query1)\n",
    "final_state1 = run1.get_final_state()\n",
    "\n",
    "# Extract final answer\n",
    "final_messages = final_state1.get(\"messages\", [])\n",
    "for msg in reversed(final_messages):\n",
    "    if isinstance(msg, AIMessage) and msg.content:\n",
    "        print(f\"\\nAnswer: {msg.content}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTotal tokens used: {final_state1.get('total_tokens', 0)}\")\n",
    "print(f\"Number of steps: {len(run1.snapshots)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a55081",
   "metadata": {},
   "source": [
    "### (Optional) Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9cc4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Update your agent with long-term memory\n",
    "# The agent already uses ShortTermMemory for conversation state\n",
    "# For long-term memory, you could use the LongTermMemory class from lib.memory\n",
    "# to store useful information from web searches for future use\n",
    "\n",
    "# Example of how to add long-term memory:\n",
    "# from lib.memory import LongTermMemory, MemoryFragment, VectorStoreManager\n",
    "# from lib.vector_db import VectorStoreManager\n",
    "# \n",
    "# memory_manager = VectorStoreManager(OPENAI_API_KEY)\n",
    "# long_term_memory = LongTermMemory(memory_manager)\n",
    "# \n",
    "# # After a web search, you could store useful information:\n",
    "# # memory_fragment = MemoryFragment(\n",
    "# #     content=\"Useful game information from web search\",\n",
    "# #     owner=\"user_id\",\n",
    "# #     namespace=\"game_info\"\n",
    "# # )\n",
    "# # long_term_memory.register(memory_fragment)\n",
    "\n",
    "print(\"Note: Long-term memory implementation is optional and can be added as needed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c475d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Query 2: \"Which one was the first 3D platformer Mario game?\"\n",
    "query2 = \"Which one was the first 3D platformer Mario game?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Query 2: {query2}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "run2 = agent.invoke(query2)\n",
    "final_state2 = run2.get_final_state()\n",
    "\n",
    "# Extract final answer\n",
    "final_messages = final_state2.get(\"messages\", [])\n",
    "for msg in reversed(final_messages):\n",
    "    if isinstance(msg, AIMessage) and msg.content:\n",
    "        print(f\"\\nAnswer: {msg.content}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTotal tokens used: {final_state2.get('total_tokens', 0)}\")\n",
    "print(f\"Number of steps: {len(run2.snapshots)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c86e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Query 3: \"Was Mortal Kombat X released for PlayStation 5?\"\n",
    "query3 = \"Was Mortal Kombat X released for PlayStation 5?\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"Query 3: {query3}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "run3 = agent.invoke(query3)\n",
    "final_state3 = run3.get_final_state()\n",
    "\n",
    "# Extract final answer\n",
    "final_messages = final_state3.get(\"messages\", [])\n",
    "for msg in reversed(final_messages):\n",
    "    if isinstance(msg, AIMessage) and msg.content:\n",
    "        print(f\"\\nAnswer: {msg.content}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nTotal tokens used: {final_state3.get('total_tokens', 0)}\")\n",
    "print(f\"Number of steps: {len(run3.snapshots)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ecb25",
   "metadata": {},
   "source": [
    "### Detailed Agent Workflow Analysis\n",
    "\n",
    "Let's examine the agent's workflow for one of the queries to see how it uses tools:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452eee34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze the workflow for query 1\n",
    "print(\"Workflow Analysis for Query 1:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, snapshot in enumerate(run1.snapshots, 1):\n",
    "    print(f\"\\nStep {i}: {snapshot.step_id}\")\n",
    "    state = snapshot.state_data\n",
    "    \n",
    "    # Show tool calls if any\n",
    "    messages = state.get(\"messages\", [])\n",
    "    for msg in messages:\n",
    "        if isinstance(msg, AIMessage) and msg.tool_calls:\n",
    "            print(f\"  Tool calls made:\")\n",
    "            for tc in msg.tool_calls:\n",
    "                print(f\"    - {tc.function.name}\")\n",
    "                try:\n",
    "                    args = json.loads(tc.function.arguments)\n",
    "                    print(f\"      Arguments: {args}\")\n",
    "                except:\n",
    "                    pass\n",
    "        elif isinstance(msg, ToolMessage):\n",
    "            print(f\"  Tool result from {msg.name}: {msg.content[:200]}...\")\n",
    "    \n",
    "    # Show final answer if available\n",
    "    if snapshot.step_id == \"__termination__\":\n",
    "        for msg in reversed(messages):\n",
    "            if isinstance(msg, AIMessage) and msg.content:\n",
    "                print(f\"\\n  Final Answer: {msg.content[:300]}...\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986b53e5",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "The UdaPlay agent is now fully implemented with:\n",
    "- ✓ retrieve_game tool: Searches internal vector database\n",
    "- ✓ evaluate_retrieval tool: Assesses retrieval quality using LLM judge\n",
    "- ✓ game_web_search tool: Performs web search using Tavily API\n",
    "- ✓ Agent with state machine workflow\n",
    "- ✓ Tested with multiple example queries\n",
    "- ✓ Structured responses with citations\n",
    "\n",
    "The agent follows a two-tier retrieval approach:\n",
    "1. First attempts to answer from internal knowledge (RAG)\n",
    "2. Evaluates the quality of retrieved results\n",
    "3. Falls back to web search if needed\n",
    "4. Combines information from all sources with proper citations\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
