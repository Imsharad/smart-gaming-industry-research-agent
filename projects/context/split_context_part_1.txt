START OF FILE: building-agents/my_submission/master_context.md
================================================================================

# Master Context for specific files in my_submission
Generated on Mon Dec 22 20:37:56 IST 2025


<file_content path="src/project/README.md">
# UdaPlay - AI Game Research Agent Project

## Project Overview
UdaPlay is an AI-powered research agent for the video game industry. This project is divided into two main parts that will help you build a sophisticated AI agent capable of answering questions about video games using both local knowledge and web searches.

## Project Structure

### Part 1: Offline RAG (Retrieval-Augmented Generation)
In this part, you'll build a Vector Database using ChromaDB to store and retrieve video game information efficiently.

Key tasks:
- Set up ChromaDB as a persistent client
- Create a collection with appropriate embedding functions
- Process and index game data from JSON files
- Each game document contains:
  - Name
  - Platform
  - Genre
  - Publisher
  - Description
  - Year of Release

### Part 2: AI Agent Development
Build an intelligent agent that combines local knowledge with web search capabilities.

The agent will have the following capabilities:
1. Answer questions using internal knowledge (RAG)
2. Search the web when needed
3. Maintain conversation state
4. Return structured outputs
5. Store useful information for future use

Required Tools to Implement:
1. `retrieve_game`: Search the vector database for game information
2. `evaluate_retrieval`: Assess the quality of retrieved results
3. `game_web_search`: Perform web searches for additional information

## Requirements

### Environment Setup
Create a `.env` file with the following API keys:
```
OPENAI_API_KEY="YOUR_KEY"
CHROMA_OPENAI_API_KEY="YOUR_KEY"
TAVILY_API_KEY="YOUR_KEY"
```

### Project Dependencies
- Python 3.11+
- ChromaDB
- OpenAI
- Tavily
- dotenv

### Directory Structure
```
project/
├── starter/
│   ├── games/           # JSON files with game data
│   ├── lib/             # Custom library implementations
│   │   ├── llm.py       # LLM abstractions
│   │   ├── messages.py  # Message handling
│   │   ├── ...
│   │   └── tooling.py   # Tool implementations
│   ├── Udaplay_01_starter_project.ipynb  # Part 1 implementation
│   └── Udaplay_02_starter_project.ipynb  # Part 2 implementation
```

## Getting Started

1. Create and activate a virtual environment
2. Install required dependencies
3. Set up your `.env` file with necessary API keys
4. Follow the notebooks in order:
   - Complete Part 1 to set up your vector database
   - Complete Part 2 to implement the AI agent

## Testing Your Implementation

After completing both parts, test your agent with questions like:
- "When was Pokémon Gold and Silver released?"
- "Which one was the first 3D platformer Mario game?"
- "Was Mortal Kombat X released for PlayStation 5?"

## Advanced Features

After completing the basic implementation, you can enhance your agent with:
- Long-term memory capabilities
- Additional tools and capabilities

## Notes
- Make sure to implement proper error handling
- Follow best practices for API key management
- Document your code thoroughly
- Test your implementation with various types of queries
</file_content>

<file_content path="src/project/Udaplay_01_starter_project.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdd0bcb",
   "metadata": {},
   "source": [
    "# [STARTER] Udaplay Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325b035",
   "metadata": {},
   "source": [
    "## Part 01 - Offline RAG\n",
    "\n",
    "In this part of the project, you'll build your VectorDB using Chroma.\n",
    "\n",
    "The data is inside folder `project/starter/games`. Each file will become a document in the collection you'll create.\n",
    "Example.:\n",
    "```json\n",
    "{\n",
    "  \"Name\": \"Gran Turismo\",\n",
    "  \"Platform\": \"PlayStation 1\",\n",
    "  \"Genre\": \"Racing\",\n",
    "  \"Publisher\": \"Sony Computer Entertainment\",\n",
    "  \"Description\": \"A realistic racing simulator featuring a wide array of cars and tracks, setting a new standard for the genre.\",\n",
    "  \"YearOfRelease\": 1997\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42de90",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61283b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only needed for Udacity workspace\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Check if 'pysqlite3' is available before importing\n",
    "if importlib.util.find_spec(\"pysqlite3\") is not None:\n",
    "    import pysqlite3\n",
    "    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d56169",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a .env file with the following variables\n",
    "# OPENAI_API_KEY=\"YOUR_KEY\"\n",
    "# CHROMA_OPENAI_API_KEY=\"YOUR_KEY\"\n",
    "# TAVILY_API_KEY=\"YOUR_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e465d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load environment variables\n",
    "# load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27de4729",
   "metadata": {},
   "source": [
    "### VectorDB Instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Instantiate your ChromaDB Client\n",
    "# Choose any path you want\n",
    "# chroma_client = chromadb.PersistentClient(path=\"chromadb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df844b3b",
   "metadata": {},
   "source": [
    "### Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c56281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pick one embedding function\n",
    "# If picking something different than openai, \n",
    "# make sure you use the same when loading it\n",
    "# embedding_fn = embedding_functions.OpenAIEmbeddingFunction()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec23893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create a collection\n",
    "# Choose any name you want\n",
    "# collection = chroma_client.create_collection(\n",
    "#    name=\"udaplay\",\n",
    "#    embedding_function=embedding_fn\n",
    "#)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a55081",
   "metadata": {},
   "source": [
    "### Add documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure you have a directory \"project/starter/games\"\n",
    "data_dir = \"games\"\n",
    "\n",
    "for file_name in sorted(os.listdir(data_dir)):\n",
    "    if not file_name.endswith(\".json\"):\n",
    "        continue\n",
    "\n",
    "    file_path = os.path.join(data_dir, file_name)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        game = json.load(f)\n",
    "\n",
    "    # You can change what text you want to index\n",
    "    content = f\"[{game['Platform']}] {game['Name']} ({game['YearOfRelease']}) - {game['Description']}\"\n",
    "\n",
    "    # Use file name (like 001) as ID\n",
    "    doc_id = os.path.splitext(file_name)[0]\n",
    "\n",
    "    collection.add(\n",
    "        ids=[doc_id],\n",
    "        documents=[content],\n",
    "        metadatas=[game]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file_content>

<file_content path="src/project/Udaplay_02_starter_project.ipynb">
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fdd0bcb",
   "metadata": {},
   "source": [
    "# [STARTER] Udaplay Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9325b035",
   "metadata": {},
   "source": [
    "## Part 02 - Agent\n",
    "\n",
    "In this part of the project, you'll use your VectorDB to be part of your Agent as a tool.\n",
    "\n",
    "You're building UdaPlay, an AI Research Agent for the video game industry. The agent will:\n",
    "1. Answer questions using internal knowledge (RAG)\n",
    "2. Search the web when needed\n",
    "3. Maintain conversation state\n",
    "4. Return structured outputs\n",
    "5. Store useful information for future use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b42de90",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a963d4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only needed for Udacity workspace\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Check if 'pysqlite3' is available before importing\n",
    "if importlib.util.find_spec(\"pysqlite3\") is not None:\n",
    "    import pysqlite3\n",
    "    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd10c06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Import the necessary libs\n",
    "# For example: \n",
    "# import os\n",
    "\n",
    "# from lib.agents import Agent\n",
    "# from lib.llm import LLM\n",
    "# from lib.messages import UserMessage, SystemMessage, ToolMessage, AIMessage\n",
    "# from lib.tooling import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e465d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Load environment variables\n",
    "# load_dotenv()\n",
    "\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce364221",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27de4729",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ab2dac",
   "metadata": {},
   "source": [
    "Build at least 3 tools:\n",
    "- retrieve_game: To search the vector DB\n",
    "- evaluate_retrieval: To assess the retrieval performance\n",
    "- game_web_search: If no good, search the web\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4f14cd",
   "metadata": {},
   "source": [
    "#### Retrieve Game Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25c36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create retrieve_game tool\n",
    "# It should use chroma client and collection you created\n",
    "# chroma_client = chromadb.PersistentClient(path=\"chromadb\")\n",
    "# collection = chroma_client.get_collection(\"udaplay\")\n",
    "# Tool Docstring:\n",
    "#    Semantic search: Finds most results in the vector DB\n",
    "#    args:\n",
    "#    - query: a question about game industry. \n",
    "#\n",
    "#    You'll receive results as list. Each element contains:\n",
    "#    - Platform: like Game Boy, Playstation 5, Xbox 360...)\n",
    "#    - Name: Name of the Game\n",
    "#    - YearOfRelease: Year when that game was released for that platform\n",
    "#    - Description: Additional details about the game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910dc945",
   "metadata": {},
   "source": [
    "#### Evaluate Retrieval Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d9d014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create evaluate_retrieval tool\n",
    "# You might use an LLM as judge in this tool to evaluate the performance\n",
    "# You need to prompt that LLM with something like:\n",
    "# \"Your task is to evaluate if the documents are enough to respond the query. \"\n",
    "# \"Give a detailed explanation, so it's possible to take an action to accept it or not.\"\n",
    "# Use EvaluationReport to parse the result\n",
    "# Tool Docstring:\n",
    "#    Based on the user's question and on the list of retrieved documents, \n",
    "#    it will analyze the usability of the documents to respond to that question. \n",
    "#    args: \n",
    "#    - question: original question from user\n",
    "#    - retrieved_docs: retrieved documents most similar to the user query in the Vector Database\n",
    "#    The result includes:\n",
    "#    - useful: whether the documents are useful to answer the question\n",
    "#    - description: description about the evaluation result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7935a26",
   "metadata": {},
   "source": [
    "#### Game Web Search Tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad698aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create game_web_search tool\n",
    "# Please use Tavily client to search the web\n",
    "# Tool Docstring:\n",
    "#    Semantic search: Finds most results in the vector DB\n",
    "#    args:\n",
    "#    - question: a question about game industry. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df844b3b",
   "metadata": {},
   "source": [
    "### Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c56281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create your Agent abstraction using StateMachine\n",
    "# Equip with an appropriate model\n",
    "# Craft a good set of instructions \n",
    "# Plug all Tools you developed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec23893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Invoke your agent\n",
    "# - When Pokémon Gold and Silver was released?\n",
    "# - Which one was the first 3D platformer Mario game?\n",
    "# - Was Mortal Kombat X realeased for Playstation 5?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a55081",
   "metadata": {},
   "source": [
    "### (Optional) Advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Update your agent with long-term memory\n",
    "# TODO: Convert the agent to be a state machine, with the tools being pre-defined nodes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
</file_content>

<file_content path="src/project/lib/__init__.py">

</file_content>

<file_content path="src/project/lib/agents.py">
from typing import TypedDict, List, Optional, Union, TypeVar
import json

from lib.state_machine import StateMachine, Step, EntryPoint, Termination, Run
from lib.llm import LLM
from lib.messages import AIMessage, UserMessage, SystemMessage, ToolMessage
from lib.tooling import Tool, ToolCall
from lib.memory import ShortTermMemory

# Define the state schema
class AgentState(TypedDict):
    user_query: str  # The current user query being processed
    instructions: str  # System instructions for the agent
    messages: List[dict]  # List of conversation messages
    current_tool_calls: Optional[List[ToolCall]]  # Current pending tool calls
    total_tokens: int  # Track the cumulative total
    
class Agent:
    def __init__(self, 
                 model_name: str,
                 instructions: str, 
                 tools: List[Tool] = None,
                 temperature: float = 0.7):
        """
        Initialize an Agent
        
        Args:
            model_name: Name/identifier of the LLM model to use
            instructions: System instructions for the agent
            tools: Optional list of tools available to the agent
            temperature: Temperature parameter for LLM (default: 0.7)
        """
        self.instructions = instructions
        self.tools = tools if tools else []
        self.model_name = model_name
        self.temperature = temperature
        
        # Initialize memory and state machine
        self.memory = ShortTermMemory()
        self.workflow = self._create_state_machine()

    def _prepare_messages_step(self, state: AgentState) -> AgentState:
        """Step logic: Prepare messages for LLM consumption"""
        messages = state.get("messages", [])
        
        # If no messages exist, start with system message
        if not messages:
            messages = [SystemMessage(content=state["instructions"])]
            
        # Add the new user message
        messages.append(UserMessage(content=state["user_query"]))
        
        return {
            "messages": messages,
            "session_id": state["session_id"]
        }

    def _llm_step(self, state: AgentState) -> AgentState:
        """Step logic: Process the current state through the LLM"""
        # Initialize LLM
        llm = LLM(
            model=self.model_name,
            temperature=self.temperature,
            tools=self.tools
        )

        response = llm.invoke(state["messages"])
        tool_calls = response.tool_calls if response.tool_calls else None

        current_total = state.get("total_tokens", 0)
        if response.token_usage:
            current_total += response.token_usage.total_tokens

        # Create AI message with content and tool calls
        ai_message = AIMessage(
            content=response.content, 
            tool_calls=tool_calls,
        )

        return {
            "messages": state["messages"] + [ai_message],
            "current_tool_calls": tool_calls,
            "session_id": state["session_id"],
            "total_tokens": current_total,
        }

    def _tool_step(self, state: AgentState) -> AgentState:
        """Step logic: Execute any pending tool calls"""
        tool_calls = state["current_tool_calls"] or []
        tool_messages = []
        
        for call in tool_calls:
            # Access tool call data correctly
            function_name = call.function.name
            function_args = json.loads(call.function.arguments)
            tool_call_id = call.id
            # Find the matching tool
            tool = next((t for t in self.tools if t.name == function_name), None)
            if tool:
                result = str(tool(**function_args))
                tool_message = ToolMessage(
                    content=json.dumps(result), 
                    tool_call_id=tool_call_id, 
                    name=function_name, 
                )
                tool_messages.append(tool_message)
        
        # Clear tool calls and add results to messages
        return {
            "messages": state["messages"] + tool_messages,
            "current_tool_calls": None,
            "session_id": state["session_id"]
        }

    def _create_state_machine(self) -> StateMachine[AgentState]:
        """Create the internal state machine for the agent"""
        machine = StateMachine[AgentState](AgentState)
        
        # Create steps
        entry = EntryPoint[AgentState]()
        message_prep = Step[AgentState]("message_prep", self._prepare_messages_step)
        llm_processor = Step[AgentState]("llm_processor", self._llm_step)
        tool_executor = Step[AgentState]("tool_executor", self._tool_step)
        termination = Termination[AgentState]()
        
        machine.add_steps([entry, message_prep, llm_processor, tool_executor, termination])
        
        # Add transitions
        machine.connect(entry, message_prep)
        machine.connect(message_prep, llm_processor)
        
        # Transition based on whether there are tool calls
        def check_tool_calls(state: AgentState) -> Union[Step[AgentState], str]:
            """Transition logic: Check if there are tool calls"""
            if state.get("current_tool_calls"):
                return tool_executor
            return termination
        
        machine.connect(llm_processor, [tool_executor, termination], check_tool_calls)
        machine.connect(tool_executor, llm_processor)  # Go back to llm after tool execution
        
        return machine

    def invoke(self, query: str, session_id: Optional[str] = None) -> Run:
        """
        Run the agent on a query
        
        Args:
            query: The user's query to process
            session_id: Optional session identifier (uses "default" if None)
            
        Returns:
            The final run object after processing
        """
        session_id = session_id or "default"

        # Create session if it doesn't exist
        self.memory.create_session(session_id)

        # Get previous messages from last run if available
        previous_messages = []
        last_run: Run = self.memory.get_last_object(session_id)
        if last_run:
            last_state = last_run.get_final_state()
            if last_state:
                previous_messages = last_state["messages"]

        initial_state: AgentState = {
            "user_query": query,
            "instructions": self.instructions,
            "messages": previous_messages,
            "current_tool_calls": None,
            "session_id": session_id,
        }

        run_object = self.workflow.run(initial_state)
        
        # Store the complete run object in memory
        self.memory.add(run_object, session_id)
        
        return run_object

    def get_session_runs(self, session_id: Optional[str] = None) -> List[Run]:
        """Get all Run objects for a session
        
        Args:
            session_id: Optional session ID (uses "default" if None)
            
        Returns:
            List of Run objects in the session
        """
        return self.memory.get_all_objects(session_id)

    def reset_session(self, session_id: Optional[str] = None):
        """Reset memory for a specific session
        
        Args:
            session_id: Optional session to reset (uses "default" if None)
        """
        self.memory.reset(session_id)
</file_content>

<file_content path="src/project/lib/documents.py">
from typing import List, Dict, Any, Optional
from dataclasses import dataclass, field
import uuid
from collections.abc import MutableSequence


@dataclass
class Document:
    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    content: str = field(default_factory=str)
    metadata: Dict[str, Any] = None

class Corpus(MutableSequence):
    def __init__(self, documents: Optional[List[Document]] = None):
        self._documents = documents or []

    def __getitem__(self, index):
        return self._documents[index]

    def __setitem__(self, index, value: Document):
        if not isinstance(value, Document):
            raise TypeError("Collection only supports Document items")
        self._documents[index] = value

    def __delitem__(self, index):
        del self._documents[index]

    def __len__(self):
        return len(self._documents)

    def insert(self, index, value: Document):
        if not isinstance(value, Document):
            raise TypeError("Collection only supports Document items")
        self._documents.insert(index, value)

    def to_dict(self) -> Dict[str, List[Any]]:
        """
        Convert the corpus to a dictionary format suitable for batch operations.
        
        This method extracts all document contents, metadata, and IDs into
        separate lists, which is the format typically expected by vector
        databases and other batch processing systems. This allows for efficient
        bulk operations on the entire corpus.
        
        Returns:
            Dict[str, List[Any]]: Dictionary containing:
                - 'contents': List of all document content strings
                - 'metadatas': List of all document metadata dictionaries
                - 'ids': List of all document ID strings
                
        Example:
            >>> corpus = Corpus([doc1, doc2])
            >>> batch_data = corpus.to_dict()
            >>> chroma_collection.add(
            ...     documents=batch_data['contents'],
            ...     metadatas=batch_data['metadatas'],
            ...     ids=batch_data['ids']
            ... )
        """
        
        # Use zip with unpacking to efficiently extract all fields
        # Handle empty corpus case by providing empty defaults
        contents, metadatas, ids = zip(*(
            (doc.content, doc.metadata, doc.id) for doc in self._documents
        )) if self._documents else ([], [], [])

        return {
            'contents': list(contents),
            'metadatas': list(metadatas),
            'ids': list(ids)
        }
</file_content>

<file_content path="src/project/lib/evaluation.py">
import json
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field

from lib.agents import AgentState
from lib.state_machine import Run
from lib.llm import LLM
from lib.messages import AIMessage, BaseMessage
from lib.parsers import PydanticOutputParser


class TaskCompletionMetrics(BaseModel):
    """Metrics for task completion evaluation"""
    task_completed: bool = Field(description="Whether the task was completed successfully")
    steps_taken: int = Field(description="Number of steps taken to complete the task")
    expected_steps: Optional[int] = Field(description="Expected number of steps", default=None)


class QualityControlMetrics(BaseModel):
    """Metrics for quality control evaluation"""
    format_correct: bool = Field(description="Whether output format is correct")
    instructions_followed: bool = Field(description="Whether prompt instructions were followed")

class ToolInteractionMetrics(BaseModel):
    """Metrics for tool interaction evaluation"""
    correct_tool_selected: bool = Field(description="Whether the right tool was chosen")
    valid_arguments: bool = Field(description="Whether tool arguments were valid")
    tool_result_useful: bool = Field(description="Whether tool returned useful results")

class SystemMetrics(BaseModel):
    """System performance metrics"""
    total_tokens: int = Field(description="Total tokens used")
    execution_time: float = Field(description="Total execution time in seconds")
    tool_call_latency: float = Field(description="Average tool call latency")
    memory_usage: Optional[float] = Field(description="Memory usage if tracked", default=None)
    cost_estimate: Optional[float] = Field(description="Estimated cost in USD", default=None)

class EvaluationResult(BaseModel):
    """Complete evaluation result"""
    task_completion: TaskCompletionMetrics
    quality_control: QualityControlMetrics
    tool_interaction: ToolInteractionMetrics
    system_metrics: SystemMetrics
    overall_score: float = Field(description="Overall evaluation score (0-1)", ge=0, le=1)
    feedback: str = Field(description="Detailed feedback and recommendations")

class TestCase(BaseModel):
    """A test case for agent evaluation"""
    id: str
    description: str
    user_query: str
    expected_tools: List[str]
    reference_answer: Optional[str] = None
    max_steps: Optional[int] = None
    context: Optional[Dict[str, Any]] = None

class JudgeEvaluation(BaseModel):
    """Structured evaluation from LLM judge"""
    task_completed: bool = Field(description="Whether the task was completed successfully")
    format_correct: bool = Field(description="Whether output format is correct")
    instructions_followed: bool = Field(description="Whether prompt instructions were followed")
    explanation: str = Field(description="Brief explanation of the evaluation")

class AgentEvaluator:
    """Comprehensive agent evaluation framework"""
    
    def __init__(self):
        self.llm_judge = LLM(model="gpt-4o-mini")
    
    def evaluate_final_response(self, 
                          test_case: TestCase, 
                          agent_response: str,
                          execution_time: float,
                          total_tokens: int) -> EvaluationResult:
        """
        Evaluate the final response from the agent (black box approach)
        """
        # Use LLM as judge to evaluate the response
        judge_prompt = f"""
        Evaluate this agent response for the given task:
        
        Task: {test_case.description}
        User Query: {test_case.user_query}
        Agent Response: {agent_response}
        Reference Answer: {test_case.reference_answer or "No reference provided"}
        
        Rate the response on:
        1. Task completion: Did it fully answer the query?
        2. Format correctness: Is the format appropriate?
        3. Instruction following: Did it follow implicit instructions?
        
        Provide your evaluation with a brief explanation.
        """
        
        # Use structured output with Pydantic model
        judge_response = self.llm_judge.invoke(
            input=judge_prompt, 
            response_format=JudgeEvaluation
        )
        
        # Parse the structured response
        parser = PydanticOutputParser(model_class=JudgeEvaluation)
        try:
            evaluation = parser.parse(judge_response)
        except Exception as e:
            print(f"Debug: Structured parsing error: {e}")
            print(f"Debug: Judge response content: {judge_response.content}")
            
            # Fallback evaluation based on simple heuristics
            has_game_info = any(keyword in agent_response.lower() 
                            for keyword in ["zelda", "breath of the wild", "score", "98", "best"])
            
            evaluation = JudgeEvaluation(
                task_completed=has_game_info,
                format_correct=len(agent_response.strip()) > 0,
                instructions_followed=has_game_info,
                explanation=f"Fallback evaluation due to parsing error: {str(e)}"
            )
        
        # Calculate scores using the structured evaluation
        task_completion = TaskCompletionMetrics(
            task_completed=evaluation.task_completed,
            steps_taken=1,  # We don't track steps in final response evaluation
            expected_steps=test_case.max_steps
        )
        
        quality_control = QualityControlMetrics(
            format_correct=evaluation.format_correct,
            instructions_followed=evaluation.instructions_followed
        )
        
        # For final response evaluation, we can't evaluate tool interaction details
        tool_interaction = ToolInteractionMetrics(
            correct_tool_selected=True,  # Assume correct if task completed
            valid_arguments=True,
            tool_result_useful=evaluation.task_completed
        )
        
        system_metrics = SystemMetrics(
            total_tokens=total_tokens,
            execution_time=execution_time,
            tool_call_latency=0.0,  # Not tracked in final response
            cost_estimate=self._estimate_cost(total_tokens)
        )
        
        # Calculate overall score
        scores = [
            1.0 if task_completion.task_completed else 0.0,
            1.0 if quality_control.format_correct else 0.0,
            1.0 if quality_control.instructions_followed else 0.0
        ]
        overall_score = sum(scores) / len(scores)
        
        return EvaluationResult(
            task_completion=task_completion,
            quality_control=quality_control,
            tool_interaction=tool_interaction,
            system_metrics=system_metrics,
            overall_score=overall_score,
            feedback=evaluation.explanation
        )
    
    def evaluate_single_step(self, 
                           agent_messages: List[BaseMessage],
                           expected_tool_calls: List[str]) -> EvaluationResult:
        """
        Evaluate a single step/decision made by the agent
        """
        # Find the last AI message with tool calls
        last_ai_message = None
        for msg in reversed(agent_messages):
            if isinstance(msg, AIMessage) and msg.tool_calls:
                last_ai_message = msg
                break
        
        if not last_ai_message or not last_ai_message.tool_calls:
            # No tool calls made
            tool_interaction = ToolInteractionMetrics(
                correct_tool_selected=False,
                valid_arguments=False,
                tool_result_useful=False
            )
            overall_score = 0.0
            feedback = "No tool calls were made when expected"
        else:
            # Evaluate the tool calls
            actual_tools = [tc.function.name for tc in last_ai_message.tool_calls]
            
            correct_tool_selected = any(tool in expected_tool_calls for tool in actual_tools)
            
            # Check if arguments are valid (basic check)
            valid_arguments = True
            try:
                for tc in last_ai_message.tool_calls:
                    json.loads(tc.function.arguments)
            except:
                valid_arguments = False
            
            tool_interaction = ToolInteractionMetrics(
                correct_tool_selected=correct_tool_selected,
                valid_arguments=valid_arguments,
                tool_result_useful=correct_tool_selected  # Assume useful if correct tool
            )
            
            overall_score = sum([
                1.0 if correct_tool_selected else 0.0,
                1.0 if valid_arguments else 0.0,
                1.0 if correct_tool_selected else 0.0
            ]) / 3.0
            
            feedback = f"Selected tools: {actual_tools}, Expected: {expected_tool_calls}"
        
        # Basic metrics for single step
        task_completion = TaskCompletionMetrics(
            task_completed=tool_interaction.correct_tool_selected,
            steps_taken=1
        )
        
        quality_control = QualityControlMetrics(
            format_correct=True,  # Assume format is correct if we got here
            instructions_followed=tool_interaction.correct_tool_selected
        )
        
        system_metrics = SystemMetrics(
            total_tokens=0,  # Not tracked in single step
            execution_time=0.0,
            tool_call_latency=0.0
        )
        
        return EvaluationResult(
            task_completion=task_completion,
            quality_control=quality_control,
            tool_interaction=tool_interaction,
            system_metrics=system_metrics,
            overall_score=overall_score,
            feedback=feedback
        )
    
    def evaluate_trajectory(self, 
                          test_case: TestCase,
                          run: Run) -> EvaluationResult:
        """
        Evaluate the entire trajectory/path taken by the agent
        """
        if not run.snapshots:
            return self._create_failed_evaluation("No execution snapshots found")
        
        final_state:AgentState = run.get_final_state()
        if not final_state:
            return self._create_failed_evaluation("No final state found")
        
        # Analyze the trajectory
        actual_steps = [
            snapshot for snapshot in run.snapshots 
            if snapshot.step_id not in ["__entry__", "__termination__"]
        ]
        steps_taken = len(actual_steps)
        messages = final_state.get("messages", [])
        total_tokens = final_state.get("total_tokens", 0)
        
        # Count tool calls in the trajectory
        tool_calls_made = []
        for msg in messages:
            if isinstance(msg, AIMessage) and msg.tool_calls:
                tool_calls_made.extend([tc.function.name for tc in msg.tool_calls])
        
        # Evaluate task completion
        expected_tools_used = any(tool in tool_calls_made for tool in test_case.expected_tools)
        within_step_limit = test_case.max_steps is None or steps_taken <= test_case.max_steps
        
        task_completion = TaskCompletionMetrics(
            task_completed=expected_tools_used and within_step_limit,
            steps_taken=steps_taken,
            expected_steps=test_case.max_steps
        )
        
        # Evaluate tool interactions
        correct_tools_used = set(tool_calls_made).intersection(set(test_case.expected_tools))
        tool_interaction = ToolInteractionMetrics(
            correct_tool_selected=len(correct_tools_used) > 0,
            valid_arguments=True,  # Assume valid if execution completed
            tool_result_useful=len(correct_tools_used) > 0
        )
        
        # Quality control - check if we have a final response
        has_final_response = any(isinstance(msg, AIMessage) and msg.content 
                               for msg in messages)
        
        quality_control = QualityControlMetrics(
            format_correct=has_final_response,
            instructions_followed=expected_tools_used
        )
        
        # System metrics
        execution_time = 0.0
        if run.end_timestamp and run.start_timestamp:
            execution_time = (run.end_timestamp - run.start_timestamp).total_seconds()
        
        system_metrics = SystemMetrics(
            total_tokens=total_tokens,
            execution_time=execution_time,
            tool_call_latency=execution_time / max(len(tool_calls_made), 1),
            cost_estimate=self._estimate_cost(total_tokens)
        )
        
        # Calculate overall score
        scores = [
            1.0 if task_completion.task_completed else 0.0,
            1.0 if quality_control.format_correct else 0.0,
            1.0 if quality_control.instructions_followed else 0.0,
            1.0 if tool_interaction.correct_tool_selected else 0.0
        ]
        overall_score = sum(scores) / len(scores)
        
        feedback = f"Trajectory: {steps_taken} steps, Tools used: {tool_calls_made}, Expected: {test_case.expected_tools}"
        
        return EvaluationResult(
            task_completion=task_completion,
            quality_control=quality_control,
            tool_interaction=tool_interaction,
            system_metrics=system_metrics,
            overall_score=overall_score,
            feedback=feedback
        )
    
    def _estimate_cost(self, total_tokens: int) -> float:
        """Estimate cost based on token usage (rough estimate for GPT-4o-mini)"""
        # Rough estimate: $0.15 per 1M input tokens, $0.60 per 1M output tokens
        # Assuming 50/50 split for simplicity
        cost_per_token = (0.15 + 0.60) / 2 / 1_000_000
        return total_tokens * cost_per_token
    
    def _create_failed_evaluation(self, reason: str) -> EvaluationResult:
        """Create a failed evaluation result"""
        return EvaluationResult(
            task_completion=TaskCompletionMetrics(task_completed=False, steps_taken=0),
            quality_control=QualityControlMetrics(format_correct=False, instructions_followed=False),
            tool_interaction=ToolInteractionMetrics(correct_tool_selected=False, valid_arguments=False, tool_result_useful=False),
            system_metrics=SystemMetrics(total_tokens=0, execution_time=0.0, tool_call_latency=0.0),
            overall_score=0.0,
            feedback=reason
        )
</file_content>

<file_content path="src/project/lib/llm.py">
from typing import List, Optional, Dict, Any
from pydantic import BaseModel
from openai import OpenAI
from lib.messages import (
    AnyMessage,
    TokenUsage,
    AIMessage,
    BaseMessage,
    UserMessage,
)
from lib.tooling import Tool


class LLM:
    def __init__(
        self,
        model: str = "gpt-4o-mini",
        temperature: float = 0.0,
        tools: Optional[List[Tool]] = None,
        api_key: Optional[str] = None
    ):
        self.model = model
        self.temperature = temperature
        
        # Handle Vocareum configuration
        import os
        base_url = os.getenv("OPENAI_API_BASE")
        if api_key:
            self.client = OpenAI(api_key=api_key, base_url=base_url) if base_url else OpenAI(api_key=api_key)
        else:
            self.client = OpenAI(base_url=base_url) if base_url else OpenAI()
            
        self.tools: Dict[str, Tool] = {
            tool.name: tool for tool in (tools or [])
        }

    def register_tool(self, tool: Tool):
        self.tools[tool.name] = tool

    def _build_payload(self, messages: List[BaseMessage]) -> Dict[str, Any]:
        payload = {
            "model": self.model,
            "temperature": self.temperature,
            "messages": [m.dict() for m in messages],
        }

        if self.tools:
            payload["tools"] = [tool.dict() for tool in self.tools.values()]
            payload["tool_choice"] = "auto"

        return payload

    def _convert_input(self, input: Any) -> List[BaseMessage]:
        if isinstance(input, str):
            return [UserMessage(content=input)]
        elif isinstance(input, BaseMessage):
            return [input]
        elif isinstance(input, list) and all(isinstance(m, BaseMessage) for m in input):
            return input
        else:
            raise ValueError(f"Invalid input type {type(input)}.")

    def invoke(self, 
               input: str | BaseMessage | List[BaseMessage],
               response_format: BaseModel = None,) -> AIMessage:
        messages = self._convert_input(input)
        payload = self._build_payload(messages)
        if response_format:
            payload.update({"response_format": response_format})
            response = self.client.beta.chat.completions.parse(**payload)
        else:
            response = self.client.chat.completions.create(**payload)
        choice = response.choices[0]
        message = choice.message

        token_usage = None
        if response.usage:
            token_usage = TokenUsage(
                prompt_tokens=response.usage.prompt_tokens,
                completion_tokens=response.usage.completion_tokens,
                total_tokens=response.usage.total_tokens
            )

        return AIMessage(
            content=message.content,
            tool_calls=message.tool_calls,
            token_usage=token_usage
        )
</file_content>

<file_content path="src/project/lib/loaders.py">
from typing import List
import pdfplumber
from lib.documents import Corpus, Document


class PDFLoader:
    """
    Document loader for extracting text content from PDF files.
    
    This class provides functionality to parse PDF documents and convert them
    into a structured format suitable for vector storage and retrieval. Each
    page of the PDF becomes a separate Document object, enabling page-level
    search and retrieval in RAG applications.
    
    The loader uses pdfplumber for robust PDF text extraction, handling:
    - Multi-page PDF documents
    - Text extraction with layout preservation
    - Automatic page numbering and identification
    - Filtering of empty or whitespace-only pages
    
    Example:
        >>> loader = PDFLoader("research_paper.pdf")
        >>> corpus = loader.load()
        >>> print(f"Loaded {len(corpus)} pages")
        >>> print(f"First page content: {corpus[0].content[:100]}...")
    """
    def __init__(self, pdf_path:str):
        self.pdf_path = pdf_path

    def load(self) -> Document:
        corpus = Corpus()

        with pdfplumber.open(self.pdf_path) as pdf:
            for num, page in enumerate(pdf.pages, start=1):
                text = page.extract_text()
                if text:
                    corpus.append(
                        Document(
                            id=str(num),
                            content=text
                        )
                    )
        return corpus
</file_content>

<file_content path="src/project/lib/memory.py">
from typing import Any, Dict, List, Optional
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import copy

from lib.documents import Document, Corpus
from lib.vector_db import VectorStoreManager,QueryResult


class SessionNotFoundError(Exception):
    """Raised when attempting to access a session that doesn't exist"""
    pass


@dataclass
class ShortTermMemory():
    """Manage the history of objects across multiple sessions"""
    sessions: Dict[str, List[Any]] = field(default_factory=lambda: {})

    def __post_init__(self):
        """Initialize the default session"""
        self.create_session("default")

    def __str__(self) -> str:
        session_ids = list(self.sessions.keys())
        return f"Memory(sessions={session_ids})"

    def __repr__(self) -> str:
        return self.__str__()

    def create_session(self, session_id: str) -> bool:
        """Create a new session
        
        Args:
            session_id: Unique identifier for the session
            
        Returns:
            bool: True if session was created, False if it already existed
        """
        if session_id in self.sessions:
            return False
        self.sessions[session_id] = []
        return True

    def delete_session(self, session_id: str) -> bool:
        """Delete a session
        
        Args:
            session_id: Session to delete
            
        Returns:
            bool: True if session was deleted, False if it didn't exist
            
        Raises:
            ValueError: If attempting to delete the default session
        """
        if session_id == "default":
            raise ValueError("Cannot delete the default session")
        if session_id not in self.sessions:
            return False
        del self.sessions[session_id]
        return True

    def _validate_session(self, session_id: str):
        """Validate that a session exists
        
        Args:
            session_id: Session ID to validate
            
        Raises:
            SessionNotFoundError: If session doesn't exist
        """
        if session_id not in self.sessions:
            raise SessionNotFoundError(f"Session '{session_id}' not found")

    def add(self, object: Any, session_id: Optional[str] = None):
        """Add a new object to the history
        
        Args:
            object: Object to add to history
            session_id: Optional session ID to add to (uses default if None)
            
        Raises:
            SessionNotFoundError: If specified session doesn't exist
        """
        session_id = session_id or "default"
        self._validate_session(session_id)
        self.sessions[session_id].append(copy.deepcopy(object))

    def get_all_objects(self, session_id: Optional[str] = None) -> List[Any]:
        """Get all objects for a session
        
        Args:
            session_id: Optional session ID (uses default if None)
            
        Returns:
            List of objects in the session
            
        Raises:
            SessionNotFoundError: If specified session doesn't exist
        """
        session_id = session_id or "default"
        self._validate_session(session_id)
        return [copy.deepcopy(obj) for obj in self.sessions[session_id]]

    def get_last_object(self, session_id: Optional[str] = None) -> Optional[Any]:
        """Get the most recent object for a session
        
        Args:
            session_id: Optional session ID (uses default if None)
            
        Returns:
            The last object in the session if it exists, None if session is empty
            
        Raises:
            SessionNotFoundError: If specified session doesn't exist
        """
        objects = self.get_all_objects(session_id)
        return objects[-1] if objects else None

    def get_all_sessions(self) -> List[str]:
        """Get all session IDs"""
        return list(self.sessions.keys())

    def reset(self, session_id: Optional[str] = None):
        """Reset memory for a specific session or all sessions
        
        Args:
            session_id: Optional session ID to reset. If None, resets all sessions.
            
        Raises:
            SessionNotFoundError: If specified session doesn't exist
        """
        if session_id is None:
            # Reset all sessions to empty lists
            for sid in self.sessions:
                self.sessions[sid] = []
        else:
            self._validate_session(session_id)
            self.sessions[session_id] = []

    def pop(self, session_id: Optional[str] = None) -> Optional[Any]:
        """Remove and return the last object from a session
        
        Args:
            session_id: Optional session ID to pop from (uses default if None)
            
        Returns:
            The last object in the session if it exists, None if session is empty
            
        Raises:
            SessionNotFoundError: If specified session doesn't exist
        """
        session_id = session_id or "default"
        self._validate_session(session_id)
        
        if not self.sessions[session_id]:
            return None
        return self.sessions[session_id].pop()

@dataclass
class MemoryFragment:
    """
    Represents a single piece of memory information stored in the long-term memory system.
    
    This class encapsulates user preferences, facts, or contextual information that can be
    retrieved later to provide personalized responses in conversational AI applications.
    
    Attributes:
        content (str): The actual memory content or information to be stored
        owner (str): Identifier for the user who owns this memory fragment
        namespace (str): Logical grouping for organizing related memories (default: "default")
        timestamp (int): Unix timestamp when the memory was created (auto-generated)
    """
    content: str
    owner: str 
    namespace: str = "default"
    timestamp: int = field(default_factory=lambda: int(datetime.now().timestamp()))


@dataclass
class MemorySearchResult:
    """
    Container for the results of a memory search operation.
    
    Encapsulates both the retrieved memory fragments and associated metadata
    such as distance scores from the vector search.
    
    Attributes:
        fragments (List[MemoryFragment]): List of memory fragments matching the search query
        metadata (Dict): Additional information about the search results (e.g., distances, scores)
    """
    fragments: List[MemoryFragment]
    metadata: Dict

@dataclass
class TimestampFilter:
    """
    Filter criteria for time-based memory searches.
    
    Allows filtering memory fragments based on when they were created,
    enabling retrieval of recent memories or memories from specific time periods.
    
    Attributes:
        greater_than_value (int, optional): Unix timestamp - only return memories created after this time
        lower_than_value (int, optional): Unix timestamp - only return memories created before this time
    """
    greater_than_value: int = None
    lower_than_value: int = None

class LongTermMemory:
    """
    Manages persistent memory storage and retrieval using vector embeddings.
    
    This class provides a high-level interface for storing and searching user memories,
    preferences, and contextual information across conversation sessions. It uses
    vector similarity search to find relevant memories based on semantic meaning.
    
    The memory system supports:
    - Multi-user memory isolation
    - Namespace-based organization
    - Time-based filtering
    - Semantic similarity search
    """
    def __init__(self, db:VectorStoreManager):
        self.vector_store = db.create_store("long_term_memory", force=True)

    def get_namespaces(self) -> List[str]:
        """
        Retrieve all unique namespaces currently stored in memory.
        
        Useful for understanding how memories are organized and for
        administrative purposes.
        
        Returns:
            List[str]: List of unique namespace identifiers
        """
        results = self.vector_store.get()
        namespaces = [r["metadatas"][0]["namespace"] for r in results]
        return namespaces

    def register(self, memory_fragment:MemoryFragment, metadata:Optional[Dict[str, str]]=None):
        """
        Store a new memory fragment in the long-term memory system.
        
        The memory is converted to a vector embedding and stored with associated
        metadata for later retrieval. Additional metadata can be provided to
        enhance searchability.
        
        Args:
            memory_fragment (MemoryFragment): The memory content to store
            metadata (Optional[Dict[str, str]]): Additional metadata to associate with the memory
        """
        complete_metadata = {
            "owner": memory_fragment.owner,
            "namespace": memory_fragment.namespace,
            "timestamp": memory_fragment.timestamp,
        }
        if metadata:
            complete_metadata.update(metadata)

        self.vector_store.add(
            Document(
                content=memory_fragment.content,
                metadata=complete_metadata,
            )
        )

    def search(self, query_text:str, owner:str, limit:int=3,
               timestamp_filter:Optional[TimestampFilter]=None, 
               namespace:Optional[str]="default") -> MemorySearchResult:
        """
        Search for relevant memories using semantic similarity.
        
        Performs a vector similarity search to find memories that are semantically
        related to the query text. Results are filtered by owner, namespace, and
        optionally by timestamp range.
        
        Args:
            query_text (str): The search query to find similar memories
            owner (str): User identifier to filter memories by ownership
            limit (int): Maximum number of results to return (default: 3)
            timestamp_filter (Optional[TimestampFilter]): Time-based filtering criteria
            namespace (Optional[str]): Namespace to search within (default: "default")
            
        Returns:
            MemorySearchResult: Container with matching memory fragments and metadata
        """

        where = {
            "$and": [
                {
                    "namespace": {
                        "$eq": namespace
                    }
                },
                {
                    "owner": {
                        "$eq": owner
                    }
                },
            ]
        }

        if timestamp_filter:
            if timestamp_filter.greater_than_value:
                where["$and"].append({
                    "timestamp": {
                        "$gt": timestamp_filter.greater_than_value,
                    }
                })
            if timestamp_filter.lower_than_value:
                where["$and"].append({
                    "timestamp": {
                        "$lt": timestamp_filter.lower_than_value,
                    }
                })

        result:QueryResult = self.vector_store.query(
            query_texts=[query_text],
            n_results=limit,
            where=where
        )

        fragments = []
        documents = result.get("documents", [[]])[0]
        metadatas = result.get("metadatas", [[]])[0]

        for content, meta in zip(documents, metadatas):
            owner = meta.get("owner")
            namespace = meta.get("namespace", "default")
            timestamp = meta.get("timestamp")

            fragment = MemoryFragment(
                content=content,
                owner=owner,
                namespace=namespace,
                timestamp=timestamp
            )

            fragments.append(fragment)
        
        result_metadata = {
            "distances": result.get("distances", [[]])[0]
        }

        return MemorySearchResult(
            fragments=fragments,
            metadata=result_metadata
        )
</file_content>

<file_content path="src/project/lib/messages.py">
from pydantic import BaseModel
from typing import Optional, Union, List, Dict, Literal

from lib.tooling import ToolCall


class BaseMessage(BaseModel):
    role: str
    content: Optional[str] = ""

    def dict(self) -> Dict:
        return dict(self)


class SystemMessage(BaseMessage):
    role: Literal["system"] = "system"


class UserMessage(BaseMessage):
    role: Literal["user"] = "user"


class ToolMessage(BaseMessage):
    role: Literal["tool"] = "tool"
    tool_call_id: str
    name: str
    content: str = ""


class TokenUsage(BaseModel):
    prompt_tokens: int = 0
    completion_tokens: int = 0
    total_tokens: int = 0


class AIMessage(BaseMessage):
    role: Literal["assistant"] = "assistant"
    content: Optional[str] = ""
    tool_calls: Optional[List[ToolCall]] = None
    token_usage: Optional[TokenUsage] = None


AnyMessage = Union[
    SystemMessage,
    UserMessage,
    AIMessage,
    ToolMessage,
]
</file_content>

<file_content path="src/project/lib/parsers.py">
import json
from typing import Any, Type
from abc import ABC, abstractmethod
from pydantic import BaseModel

from lib.messages import AIMessage


class OutputParser(BaseModel, ABC):
    @abstractmethod
    def parse(self, ai_message: AIMessage) -> Any:
        pass


class StrOutputParser(OutputParser):
    def parse(self, ai_message: AIMessage) -> str:
        return ai_message.content


class ToolOutputParser(BaseModel):
    def parse(self, ai_message: AIMessage) -> list[dict]:
        return [{
            "tool_call_id":call.id,
            "args":json.loads(call.function.arguments),
            "function_name": call.function.name,
        } for call in ai_message.tool_calls]


class JsonOutputParser(OutputParser):
    def parse(self, ai_message: AIMessage) -> Any:
        return json.loads(ai_message.content)


class PydanticOutputParser(OutputParser):
    model_class: Type[BaseModel]

    def parse(self, ai_message: AIMessage) -> BaseModel:
        return self.model_class.model_validate_json(ai_message.content)
    
</file_content>

<file_content path="src/project/lib/rag.py">
from typing import TypedDict, List
import logging

from lib.state_machine import StateMachine, Step, EntryPoint, Termination, Run, Resource
from lib.llm import LLM
from lib.messages import BaseMessage, UserMessage, SystemMessage
from lib.vector_db import VectorStore


logging.getLogger('pdfminer').setLevel(logging.ERROR)


class RAGState(TypedDict):
    """
    Type definition for the state object passed through the RAG pipeline.
    """
    messages: List[BaseMessage]
    question: str
    documents: List[str]
    distances: List[float]
    answer: str

class RAG:
    """
    Retrieval-Augmented Generation (RAG) system implementation.
    
    This class orchestrates the complete RAG pipeline using a state machine approach:
    1. Retrieve: Find relevant documents using vector similarity search
    2. Augment: Combine retrieved context with the user's question
    3. Generate: Use an LLM to produce an answer based on the augmented prompt
    
    The RAG pattern enhances LLM responses by providing relevant external knowledge,
    reducing hallucinations and improving factual accuracy.
    """
    def __init__(self, llm: LLM, vector_store: VectorStore):
        self.workflow = self._create_state_machine()
        self.resource = Resource(
            vars = {
                "llm": llm,
                "vector_store": vector_store,
            }
        )

    def _retrieve(self, state:RAGState, resource:Resource) -> RAGState:
        question = state["question"]
        vector_store:VectorStore = resource.vars.get("vector_store")
        results = vector_store.query(query_texts=[question])

        documents = results['documents'][0] if results['documents'] else []
        distances = results['distances'][0] if results['distances'] else []
        
        return {"documents": documents, "distances": distances}

    def _augment(self, state:RAGState) -> RAGState:
        question = state["question"]
        documents = state["documents"]
        context = "\n\n".join(documents)

        messages = [
            SystemMessage(content="You are an assistant for question-answering tasks."),
            UserMessage(
                content=(
                    "Use the following pieces of retrieved context to answer the question. "
                    "If you don't know the answer, just say that you don't know. "
                    f"\n# Question: \n-> {question} "
                    f"\n# Context: \n-> {context} "
                    "\n# Answer: "
                )
            )
        ]

        return {"messages": messages}

    def _generate(self, state:RAGState, resource:Resource) -> RAGState:
        llm:LLM = resource.vars.get("llm")
        ai_message = llm.invoke(state["messages"])
        return {
            "answer": ai_message.content, 
            "messages": state["messages"] + [ai_message],
        }

    def _create_state_machine(self) -> StateMachine[RAGState]:
        machine = StateMachine[RAGState](RAGState)

        # Create steps
        entry = EntryPoint[RAGState]()
        retrieve = Step[RAGState]("retrieve", self._retrieve)
        augment = Step[RAGState]("augment", self._augment)
        generate = Step[RAGState]("generate", self._generate)
        termination = Termination[RAGState]()

        machine.add_steps([entry, retrieve, augment, generate, termination])
        machine.connect(entry, retrieve)
        machine.connect(retrieve, augment)
        machine.connect(augment, generate)
        machine.connect(generate, termination)

        return machine

    def invoke(self, query: str) -> Run:
        """
        Execute the complete RAG pipeline for a given query.
        
        This is the main entry point for the RAG system. It initializes the
        pipeline state with the user's question and executes the complete
        retrieve-augment-generate workflow.
        
        Args:
            query (str): The user's question or search query
            
        Returns:
            Run: Execution object containing the final state and pipeline results
            
        Example:
            >>> rag = RAG(llm, vector_store)
            >>> result = rag.invoke("What is machine learning?")
            >>> answer = result.get_final_state()["answer"]
        """
        
        initial_state: RAGState = {
            "question": query,
        }
        run_object = self.workflow.run(
            state = initial_state, 
            resource = self.resource,
        )
        return run_object
</file_content>

<file_content path="src/project/lib/state_machine.py">
from typing import Any, Callable, Dict, List, Optional, Union, TypeVar, Generic, cast, Type, TypedDict, get_type_hints
from dataclasses import dataclass, field
from datetime import datetime
import uuid
import copy
import inspect


StateSchema = TypeVar("StateSchema")

@dataclass
class Resource:
    vars: Dict[str, Any]

class Step(Generic[StateSchema]):
    def __init__(self, step_id: str, logic: Callable[[StateSchema], Dict]):
        self.step_id = step_id
        self.logic = logic
        # Store the number of parameters the logic function expects
        self.logic_params_count = self._calculate_params_count()

    def __str__(self) -> str:
        return f"Step('{self.step_id}')"

    def __repr__(self) -> str:
        return self.__str__()

    def _calculate_params_count(self):
        """Calculate the number of parameters excluding 'self' for bound methods"""
        if inspect.ismethod(self.logic):
            # For bound methods, subtract 1 to exclude 'self'
            return self.logic.__func__.__code__.co_argcount - 1
        else:
            # For regular functions
            return self.logic.__code__.co_argcount

    def run(self, state: StateSchema, state_schema: Type[StateSchema], resource: Resource=None) -> StateSchema:
        # Call logic function with appropriate number of arguments
        if self.logic_params_count == 1:
            result = self.logic(state)
        elif self.logic_params_count == 2:
            result = self.logic(state, resource)
        else:
            raise ValueError(
                f"Step '{self.step_id}' logic function must accept either 1 argument (state) "
                f"or 2 arguments (state, resource). Found {self.logic_params_count} arguments."
            ) 
        # Get expected fields from the TypedDict
        expected_fields = get_type_hints(state_schema)
        
        # Create new state with all fields from state_schema
        # Only copy fields that are defined in state_schema
        updated = {**state}
        for field, value in result.items():
            if field in expected_fields:
                updated[field] = value
        
        return cast(StateSchema, updated)


class EntryPoint(Step[StateSchema]):
    """Special step that marks the beginning of the workflow.
    Users should connect this step to their first business logic step."""
    def __init__(self):
        super().__init__("__entry__", lambda x: {})


class Termination(Step[StateSchema]):
    """Special step that marks the end of the workflow.
    Users should connect their final business logic step(s) to this step."""
    def __init__(self):
        super().__init__("__termination__", lambda x: {})


@dataclass
class Transition(Generic[StateSchema]):
    source: str
    targets: List[str]
    condition: Optional[Callable[[StateSchema], Union[str, List[str], Step[StateSchema], List[Step[StateSchema]]]]] = None

    def __str__(self) -> str:
        return f"Transition('{self.source}' -> {self.targets})"

    def __repr__(self) -> str:
        return self.__str__()

    def resolve(self, state: StateSchema) -> List[str]:
        if self.condition:
            result = self.condition(state)
            if isinstance(result, Step):
                return [result.step_id]
            elif isinstance(result, list) and all(isinstance(x, Step) for x in result):
                return [step.step_id for step in result]
            elif isinstance(result, str):
                return [result]
            return result
        return self.targets


@dataclass
class Snapshot(Generic[StateSchema]):
    """Represents a single state snapshot in time"""
    snapshot_id: str
    timestamp: datetime
    state_data: StateSchema
    state_schema: Type[StateSchema]
    step_id: str

    def __str__(self) -> str:
        return f"Snapshot('{self.snapshot_id}') @ [{self.timestamp.strftime('%Y-%m-%d %H:%M:%S.%f')}]: {self.step_id}.State({self.state_data})"

    def __repr__(self) -> str:
        return self.__str__()

    @classmethod
    def create(cls, state_data: StateSchema, state_schema: Type[StateSchema],
               step_id:str) -> 'Snapshot[StateSchema]':
        return cls(
            snapshot_id=str(uuid.uuid4()),
            timestamp=datetime.now(),
            state_data=state_data,
            state_schema=state_schema,
            step_id=step_id,
        )


@dataclass
class Run(Generic[StateSchema]):
    """Represents a single execution run of the state machine"""
    run_id: str
    start_timestamp: datetime
    snapshots: List[Snapshot[StateSchema]] = field(default_factory=list)
    end_timestamp: Optional[datetime] = None

    def __str__(self) -> str:
        return f"Run('{self.run_id}')"

    def __repr__(self) -> str:
        return self.__str__()

    @classmethod
    def create(cls) -> 'Run[StateSchema]':
        return cls(
            run_id=str(uuid.uuid4()),
            start_timestamp=datetime.now()
        )

    @property
    def metadata(self) -> Dict:
        return {
            "run_id": self.run_id,
            "start_timestamp": self.start_timestamp.strftime("%Y-%m-%d %H:%M:%S.%f"),
            "end_timestamp": self.end_timestamp.strftime("%Y-%m-%d %H:%M:%S.%f"),
            "snapshot_counts": len(self.snapshots)
        }

    def add_snapshot(self, snapshot: Snapshot[StateSchema]):
        """Add a new snapshot to this run"""
        self.snapshots.append(snapshot)

    def complete(self):
        """Mark this run as complete"""
        self.end_timestamp = datetime.now()

    def get_final_state(self) -> Optional[StateSchema]:
        """Get the final state of this run"""
        if not self.snapshots:
            return None
        return self.snapshots[-1].state_data


class StateMachine(Generic[StateSchema]):
    def __init__(self, state_schema: Type[StateSchema]):
        self.state_schema = state_schema
        self.steps: Dict[str, Step[StateSchema]] = {}
        self.transitions: Dict[str, List[Transition[StateSchema]]] = {}

    def __str__(self) -> str:
        schema_keys = list(get_type_hints(self.state_schema).keys())
        return f"StateMachine(schema={schema_keys})"

    def __repr__(self) -> str:
        return self.__str__()

    def add_steps(self, steps: List[Step[StateSchema]]):
        """Add steps to the workflow"""
        for step in steps:
            self.steps[step.step_id] = step

    def connect(
        self,
        source: Union[Step[StateSchema], str],
        targets: Union[Step[StateSchema], str, List[Union[Step[StateSchema], str]]],
        condition: Optional[Callable[[StateSchema], Union[str, List[str]]]] = None
    ):
        src_id = source.step_id if isinstance(source, Step) else source
        target_list = targets if isinstance(targets, list) else [targets]
        target_ids = [t.step_id if isinstance(t, Step) else t for t in target_list]
        transition = Transition[StateSchema](source=src_id, targets=target_ids, condition=condition)
        if src_id not in self.transitions:
            self.transitions[src_id] = []
        self.transitions[src_id].append(transition)

    def run(self, state: StateSchema, resource: Resource = None):
        # Validate that state has at least one field from the schema
        expected_fields = get_type_hints(self.state_schema)
        state_fields = set(state.keys())
        common_fields = state_fields.intersection(expected_fields)
        
        if not common_fields:
            raise ValueError(f"Initial state must have at least one field from the schema. Expected fields: {list(expected_fields.keys())}")

        entry_points = [s for s in self.steps.values() if isinstance(s, EntryPoint)]
        if not entry_points:
            raise Exception("No EntryPoint step found in workflow")
        if len(entry_points) > 1:
            raise Exception("Multiple EntryPoint steps found in workflow")
        
        # Create a new run for this execution
        current_run = Run.create()
        
        current_step_id = entry_points[0].step_id        

        while current_step_id:
            step = self.steps[current_step_id]
            if isinstance(step, Termination):
                print(f"[StateMachine] Terminating: {current_step_id}")
                break
            
            # Replace state entirely
            state = step.run(state, self.state_schema, resource)  

            if isinstance(step, EntryPoint):
                print(f"[StateMachine] Starting: {current_step_id}")
            else:
                print(f"[StateMachine] Executing step: {current_step_id}")

            # Create and add snapshot to the current run
            snapshot = Snapshot.create(copy.deepcopy(state), self.state_schema, current_step_id)
            current_run.add_snapshot(snapshot)

            transitions = self.transitions.get(current_step_id, [])
            next_steps: List[str] = []

            for t in transitions:
                next_steps += t.resolve(state)

            if not next_steps:
                raise Exception(f"[StateMachine] No transitions found from step: {current_step_id}")

            if len(next_steps) > 1:
                raise NotImplementedError("Parallel execution not implemented yet.")

            current_step_id = next_steps[0]

        current_run.complete()
        return current_run
</file_content>

<file_content path="src/project/lib/tooling.py">
import inspect
import datetime
from typing import (
    Any, Callable, 
    Literal, Optional, Union, TypeAlias,
    get_type_hints, get_origin, get_args,
)
from functools import wraps
from openai.types.chat.chat_completion_message_tool_call import ChatCompletionMessageToolCall


# Type alias for OpenAI's tool call implementation
ToolCall: TypeAlias = ChatCompletionMessageToolCall

class Tool:
    def __init__(
        self,
        func: Callable,
        name: Optional[str] = None,
        description: Optional[str] = None
    ):
        self.func = func
        self.name = name or func.__name__
        self.description = description or inspect.getdoc(func)
        self.signature = inspect.signature(func, eval_str=True)
        self.type_hints = get_type_hints(func)

        self.parameters = [
            self._build_param_schema(key, param)
            for key, param in self.signature.parameters.items()
        ]

    def _build_param_schema(self, name: str, param: inspect.Parameter):
        param_type = self.type_hints.get(name, str)
        schema = self._infer_json_schema_type(param_type)
        return {
            "name": name,
            "schema": schema,
            "required": param.default == inspect.Parameter.empty
        }

    def _infer_json_schema_type(self, typ: Any) -> dict:
        origin = get_origin(typ)

        # Handle Literal (enums)
        if origin is Literal:
            return {
                "type": "string",
                "enum": list(get_args(typ))
            }

        # Handle Optional[T]
        if origin is Union:
            args = get_args(typ)
            non_none = [arg for arg in args if arg is not type(None)]
            if len(non_none) == 1:
                return self._infer_json_schema_type(non_none[0])
            return {"type": "string"}  # fallback

        # Handle collections
        if origin is list:
            return {
                "type": "array",
                "items": self._infer_json_schema_type(get_args(typ)[0] if get_args(typ) else str)
            }

        if origin is dict:
            return {
                "type": "object",
                "additionalProperties": self._infer_json_schema_type(get_args(typ)[1] if get_args(typ) else str)
            }

        # Primitive mappings
        mapping = {
            str: "string",
            int: "integer",
            float: "number",
            bool: "boolean",
            datetime.date: "string",
            datetime.datetime: "string",
        }

        return {"type": mapping.get(typ, "string")}

    def dict(self) -> dict:
        return {
            "type": "function",
            "function": {
                "name": self.name,
                "description": self.description,
                "parameters": {
                    "type": "object",
                    "properties": {
                        param["name"]: param["schema"]
                        for param in self.parameters
                    },
                    "required": [
                        param["name"] for param in self.parameters if param["required"]
                    ],
                    "additionalProperties": False
                }
            }
        }

    def __call__(self, *args, **kwargs):
        return self.func(*args, **kwargs)

    def __repr__(self):
        return f"<Tool name={self.name} params={[p['name'] for p in self.parameters]}>"

    @classmethod
    def from_func(cls, func: Callable):
        return cls(func)



def tool(func=None, *, name: str = None, description: str = None):
    def wrapper(f):
        @wraps(f)
        def wrapped(*args, **kwargs):
            return f(*args, **kwargs)
        return Tool(f, name=name, description=description)
    
    # @tool ou @tool(name="foo")
    return wrapper(func) if func else wrapper
</file_content>

<file_content path="src/project/lib/vector_db.py">
from typing import List, Optional, Dict, Any, Union
from typing_extensions import TypedDict
import chromadb
from chromadb.utils import embedding_functions
from chromadb.api.models.Collection import Collection as ChromaCollection
from chromadb.api.types import EmbeddingFunction, QueryResult, GetResult

from lib.loaders import PDFLoader
from lib.documents import Document, Corpus


class VectorStore:
    """
    High-level interface for vector database operations using ChromaDB.
    
    This class provides a simplified API for storing and querying document embeddings
    in a ChromaDB collection. It handles the conversion between our Document/Corpus
    abstractions and ChromaDB's expected data formats, making vector operations
    more intuitive and type-safe.
    
    The VectorStore supports:
    - Adding individual documents, document lists, or corpus collections
    - Semantic similarity search with filtering capabilities
    - Metadata-based document retrieval
    - Automatic embedding generation via OpenAI
    """

    def __init__(self, chroma_collection: ChromaCollection):
        self._collection = chroma_collection

    def add(self, item: Union[Document, Corpus, List[Document]]):
        """
        Add documents to the vector store with automatic embedding generation.
        
        This method accepts various input formats and normalizes them to the
        ChromaDB batch format. Documents are automatically embedded using the
        collection's configured embedding function (typically OpenAI).
        
        Args:
            item (Union[Document, Corpus, List[Document]]): Documents to add.
                Can be a single Document, a Corpus collection, or a list of Documents.
                
        Raises:
            TypeError: If the input type is not supported or if a list contains
                non-Document objects.
                
        Example:
            >>> store.add(Document(content="AI is transforming healthcare"))
            >>> store.add([doc1, doc2, doc3])  # Batch add
            >>> store.add(Corpus([doc1, doc2]))  # Add corpus
        """
        if isinstance(item, Document):
            item = Corpus([item])
        elif isinstance(item, list):
            if not all(isinstance(doc, Document) for doc in item):
                raise TypeError("List must contain Document objects only.")
            item = Corpus(item)
        elif not isinstance(item, Corpus):
            raise TypeError("item must be Document, Corpus, or List[Document].")

        item_dict = item.to_dict()

        self._collection.add(
            documents=item_dict["contents"],
            ids=item_dict["ids"],
            metadatas=item_dict["metadatas"]
        )

    def query(self, query_texts: str | List[str], n_results: int = 3,
              where: Optional[Dict[str, Any]] = None,
              where_document: Optional[Dict[str, Any]] = None) -> QueryResult:
        """
        Perform semantic similarity search against stored documents.
        
        This method finds documents that are semantically similar to the query
        text using vector embeddings. Results are ranked by cosine similarity
        and can be filtered using metadata or document content conditions.
        
        Args:
            query_texts (List[str]): List of query strings to search for
            n_results (int): Maximum number of results to return per query (default: 3)
            where (Optional[Dict[str, Any]]): Metadata filter conditions using
                ChromaDB query syntax (e.g., {"author": "Smith"})
            where_document (Optional[Dict[str, Any]]): Document content filter
                conditions using ChromaDB query syntax
                
        Returns:
            QueryResult: ChromaDB query result containing documents, distances,
                metadata, and IDs for the most similar documents
                
        Example:
            >>> results = store.query(
            ...     query_texts=["machine learning algorithms"],
            ...     n_results=5,
            ...     where={"category": "technical"}
            ... )
            >>> for doc, distance in zip(results['documents'][0], results['distances'][0]):
            ...     print(f"Similarity: {1-distance:.3f}, Content: {doc[:100]}...")
        """
        return self._collection.query(
            query_texts=query_texts,
            n_results=n_results,
            where=where,
            where_document=where_document,
            include=['documents', 'distances', 'metadatas']
        )

    def get(self, ids: Optional[List[str]] = None, 
            where: Optional[Dict[str, Any]] = None,
            limit: Optional[int] = None) -> GetResult:
        """
        Retrieve documents by ID or metadata filters without similarity search.
        
        This method performs direct document retrieval based on exact ID matches
        or metadata filter conditions, without computing embedding similarities.
        Useful for fetching specific documents or browsing collections.
        
        Args:
            ids (Optional[List[str]]): Specific document IDs to retrieve
            where (Optional[Dict[str, Any]]): Metadata filter conditions
            limit (Optional[int]): Maximum number of documents to return
            
        Returns:
            GetResult: ChromaDB result containing the requested documents
                with their metadata and IDs
                
        Example:
            >>> # Get specific documents by ID
            >>> docs = store.get(ids=["doc-123", "doc-456"])
            >>> 
            >>> # Get all documents from a specific source
            >>> docs = store.get(where={"source": "research_papers"}, limit=10)
        """
        return self._collection.get(
            ids=ids,
            where=where,
            limit=limit,
            include=["documents", "metadatas"]
        )

class VectorStoreManager:
    """
    Factory and lifecycle manager for ChromaDB vector stores.
    
    This class handles the creation, configuration, and management of ChromaDB
    collections with OpenAI embeddings. It provides a centralized way to manage
    multiple vector stores within an application, handling the underlying ChromaDB
    client and embedding function configuration.
    
    Key responsibilities:
    - ChromaDB client initialization and management
    - OpenAI embedding function configuration
    - Vector store creation with consistent settings
    - Store lifecycle management (create, get, delete)
    """

    def __init__(self, openai_api_key: str):
        self.chroma_client = chromadb.Client()
        self.embedding_function = self._create_embedding_function(openai_api_key)

    def _create_embedding_function(self, api_key: str) -> EmbeddingFunction:
        embeddings_fn = embedding_functions.OpenAIEmbeddingFunction(
            api_key=api_key
        )
        return embeddings_fn

    def __repr__(self):
        return f"VectorStoreManager():{self.chroma_client}"

    def get_store(self, name: str) -> Optional[VectorStore]:
        try:
            chroma_collection = self.chroma_client.get_collection(name)
            return VectorStore(chroma_collection)
        except Exception:
            return None

    def create_store(self, store_name: str, force: bool = False) -> VectorStore:
        if force:
            self.delete_store(store_name)

        try:
            chroma_collection = self.chroma_client.create_collection(
                name=store_name,
                embedding_function=self.embedding_function
            )
        except Exception as e:
            print(f"Pass `force=True` or use `get_or_create_store` method")

        return VectorStore(chroma_collection)

    def get_or_create_store(self, store_name: str) -> VectorStore:
        chroma_collection = self.chroma_client.get_or_create_collection(
            name=store_name,
            embedding_function=self.embedding_function
        )
        return VectorStore(chroma_collection)

    def delete_store(self, store_name: str):
        try:
            self.chroma_client.delete_collection(name=store_name)
        except Exception:
            pass  # Store doesn't exist yet


class CorpusLoaderService:
    """
    Service for loading documents from various sources into vector stores.
    
    This class provides convenient methods for loading and processing documents
    from different file formats (currently PDF) into vector stores. It handles
    the entire pipeline from file loading to vector store insertion.
    
    The service abstracts away the complexity of:
    - Document loading and parsing
    - Vector store creation and management
    - Batch document insertion
    - Progress reporting and error handling
    """

    def __init__(self, vector_store_manager: VectorStoreManager):
        self.manager = vector_store_manager

    def load_pdf(self, store_name: str, pdf_path: str) -> VectorStore:
        """
        Load a PDF file into a vector store.
        
        This method handles the complete pipeline of loading a PDF document,
        parsing its content into pages/chunks, and storing them in a vector
        store with embeddings. Each page becomes a separate document in the store.
        
        Args:
            store_name (str): Name of the vector store to create or use
            pdf_path (str): Path to the PDF file to load
            
        Returns:
            VectorStore: The vector store containing the loaded PDF content
            
        Example:
            >>> loader = CorpusLoaderService(manager)
            >>> store = loader.load_pdf("research_papers", "paper.pdf")
            >>> # PDF is now searchable in the vector store
            >>> results = store.query(["machine learning methodology"])
        """
        store = self.manager.get_or_create_store(store_name)
        print(f"VectorStore `{store_name}` ready!")

        loader = PDFLoader(pdf_path)
        document = loader.load()
        store.add(document)
        print(f"Pages from `{pdf_path}` added!")

        return store
</file_content>

<file_content path="src/project/games/001.json">
{
  "Name": "Gran Turismo",
  "Platform": "PlayStation 1",
  "Genre": "Racing",
  "Publisher": "Sony Computer Entertainment",
  "Description": "A realistic racing simulator featuring a wide array of cars and tracks, setting a new standard for the genre.",
  "YearOfRelease": 1997
}
</file_content>

<file_content path="src/project/games/002.json">
{
  "Name": "Grand Theft Auto: San Andreas",
  "Platform": "PlayStation 2",
  "Genre": "Action-adventure",
  "Publisher": "Rockstar Games",
  "Description": "An expansive open-world game set in the fictional state of San Andreas, following the story of Carl 'CJ' Johnson.",
  "YearOfRelease": 2004
}
</file_content>

<file_content path="src/project/games/003.json">
{
  "Name": "Gran Turismo 5",
  "Platform": "PlayStation 3",
  "Genre": "Racing",
  "Publisher": "Sony Computer Entertainment",
  "Description": "A comprehensive racing simulator featuring a vast selection of vehicles and tracks, with realistic driving physics.",
  "YearOfRelease": 2010
}
</file_content>

<file_content path="src/project/games/004.json">
{
  "Name": "Marvel's Spider-Man",
  "Platform": "PlayStation 4",
  "Genre": "Action-adventure",
  "Publisher": "Sony Interactive Entertainment",
  "Description": "An open-world superhero game that lets players swing through New York City as Spider-Man, battling iconic villains.",
  "YearOfRelease": 2018
}
</file_content>

<file_content path="src/project/games/005.json">
{
  "Name": "Marvel's Spider-Man 2",
  "Platform": "PlayStation 5",
  "Genre": "Action-adventure",
  "Publisher": "Sony Interactive Entertainment",
  "Description": "The sequel to the acclaimed Spider-Man game, featuring both Peter Parker and Miles Morales as playable characters.",
  "YearOfRelease": 2023
}
</file_content>

<file_content path="src/project/games/006.json">
{
  "Name": "Pokémon Gold and Silver",
  "Platform": "Game Boy Color",
  "Genre": "Role-playing",
  "Publisher": "Nintendo",
  "Description": "Second-generation Pokémon games introducing new regions, Pokémon, and gameplay mechanics.",
  "YearOfRelease": 1999
}
</file_content>

<file_content path="src/project/games/007.json">
{
  "Name": "Pokémon Ruby and Sapphire",
  "Platform": "Game Boy Advance",
  "Genre": "Role-playing",
  "Publisher": "Nintendo",
  "Description": "Third-generation Pokémon games set in the Hoenn region, featuring new Pokémon and double battles.",
  "YearOfRelease": 2002
}
</file_content>

<file_content path="src/project/games/008.json">
{
  "Name": "Super Mario World",
  "Platform": "Super Nintendo Entertainment System (SNES)",
  "Genre": "Platformer",
  "Publisher": "Nintendo",
  "Description": "A classic platformer where Mario embarks on a quest to save Princess Toadstool and Dinosaur Land from Bowser.",
  "YearOfRelease": 1990
}
</file_content>

<file_content path="src/project/games/009.json">
{
  "Name": "Super Mario 64",
  "Platform": "Nintendo 64",
  "Genre": "Platformer",
  "Publisher": "Nintendo",
  "Description": "A groundbreaking 3D platformer that set new standards for the genre, featuring Mario's quest to rescue Princess Peach.",
  "YearOfRelease": 1996
}
</file_content>

<file_content path="src/project/games/010.json">
{
  "Name": "Super Smash Bros. Melee",
  "Platform": "GameCube",
  "Genre": "Fighting",
  "Publisher": "Nintendo",
  "Description": "A crossover fighting game featuring characters from various Nintendo franchises battling it out in dynamic arenas.",
  "YearOfRelease": 2001
}
</file_content>

<file_content path="src/project/games/011.json">
{
  "Name": "Wii Sports",
  "Platform": "Wii",
  "Genre": "Sports",
  "Publisher": "Nintendo",
  "Description": "A collection of sports games that utilize the Wii's motion controls, bundled with the console to showcase its capabilities.",
  "YearOfRelease": 2006
}
</file_content>

<file_content path="src/project/games/012.json">
{
  "Name": "Mario Kart 8 Deluxe",
  "Platform": "Nintendo Switch",
  "Genre": "Racing",
  "Publisher": "Nintendo",
  "Description": "An enhanced version of Mario Kart 8, featuring new characters, tracks, and improved gameplay mechanics.",
  "YearOfRelease": 2017
}
</file_content>

<file_content path="src/project/games/013.json">
{
  "Name": "Kinect Adventures!",
  "Platform": "Xbox 360",
  "Genre": "Party",
  "Publisher": "Microsoft Game Studios",
  "Description": "A collection of mini-games designed to showcase the capabilities of the Kinect motion sensor.",
  "YearOfRelease": 2010
}
</file_content>

<file_content path="src/project/games/014.json">
{
  "Name": "Minecraft",
  "Platform": "Xbox One",
  "Genre": "Sandbox, Survival",
  "Publisher": "Mojang Studios",
  "Description": "A sandbox game that allows players to build and explore infinite worlds, fostering creativity and adventure.",
  "YearOfRelease": 2014
}
</file_content>

<file_content path="src/project/games/015.json">
{
  "Name": "Halo Infinite",
  "Platform": "Xbox Series X|S",
  "Genre": "First-person shooter",
  "Publisher": "Xbox Game Studios",
  "Description": "The latest installment in the Halo franchise, featuring Master Chief's return in a new open-world setting.",
  "YearOfRelease": 2021
}
</file_content>

<file_content path="Udaplay_01_solution_project.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Udaplay 01 \u2013 RAG Setup with ChromaDB\n",
        "\n",
        "This notebook loads game JSON files from `games/`, builds a ChromaDB vector store, and demonstrates semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current working directory: /workspace/Code/project/starter\nFiles in games/: ['001.json', '002.json', '003.json', '004.json', '005.json', '006.json', '007.json', '008.json', '009.json', '010.json', '011.json', '012.json', '013.json', '014.json', '015.json']\n"
          ]
        }
      ],
      "source": [
        "import os, json\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n",
        "\n",
        "print(\"Current working directory:\", os.getcwd())\n",
        "print(\"Files in games/:\", sorted(os.listdir(\"games\")))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample file: 001.json\nGame keys: ['Name', 'Platform', 'Genre', 'Publisher', 'Description', 'YearOfRelease']\nSample game record: {'Name': 'Gran Turismo', 'Platform': 'PlayStation 1', 'Genre': 'Racing', 'Publisher': 'Sony Computer Entertainment', 'Description': 'A realistic racing simulator featuring a wide array of cars and tracks, setting a new standard for the genre.', 'YearOfRelease': 1997}\n"
          ]
        }
      ],
      "source": [
        "data_dir = \"games\"\n",
        "\n",
        "file_name = sorted(os.listdir(data_dir))[0]\n",
        "file_path = os.path.join(data_dir, file_name)\n",
        "\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    game = json.load(f)\n",
        "\n",
        "print(\"Sample file:\", file_name)\n",
        "print(\"Game keys:\", list(game.keys()))\n",
        "print(\"Sample game record:\", game)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChromaDB client initialized. Collection name: games\n"
          ]
        }
      ],
      "source": [
        "# Create a persistent ChromaDB client and collection\n",
        "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
        "embedding_fn = embedding_functions.DefaultEmbeddingFunction()\n",
        "collection = client.get_or_create_collection(name=\"games\", embedding_function=embedding_fn)\n",
        "\n",
        "print(\"ChromaDB client initialized. Collection name:\", collection.name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexed 15 games into the 'games' collection.\n"
          ]
        }
      ],
      "source": [
        "# Index all game JSON files into the collection\n",
        "data_dir = \"games\"\n",
        "ids, documents, metadatas = [], [], []\n",
        "\n",
        "for file_name in sorted(os.listdir(data_dir)):\n",
        "    if not file_name.endswith(\".json\"):\n",
        "        continue\n",
        "\n",
        "    file_path = os.path.join(data_dir, file_name)\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        game = json.load(f)\n",
        "\n",
        "    content = f\"[{game['Platform']}] {game['Name']} ({game['YearOfRelease']}) - {game['Description']}\"\n",
        "    doc_id = os.path.splitext(file_name)[0]\n",
        "\n",
        "    ids.append(doc_id)\n",
        "    documents.append(content)\n",
        "    metadatas.append(game)\n",
        "\n",
        "collection.add(ids=ids, documents=documents, metadatas=metadatas)\n",
        "print(f\"Indexed {len(ids)} games into the 'games' collection.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query: Who developed FIFA 21?\n\nResult 1:\n  Name          : FIFA 21\n  Platform      : PlayStation 4\n  YearOfRelease : 2020\n  Publisher     : Electronic Arts\n  Similarity    : 0.12\n  Snippet       : [PlayStation 4] FIFA 21 (2020) - A football simulation game featuring realistic gameplay and updated squads....\n\nResult 2:\n  Name          : Sample Game 6\n  Platform      : Multi-platform\n  YearOfRelease : 2006\n  Publisher     : Sample Publisher\n  Similarity    : 0.34\n  Snippet       : [Multi-platform] Sample Game 6 (2006) - A sample description for game 6 used for testing the RAG pipeline....\n"
          ]
        }
      ],
      "source": [
        "# Demonstrate semantic search over the vector database\n",
        "query = \"Who developed FIFA 21?\"\n",
        "\n",
        "results = collection.query(\n",
        "    query_texts=[query],\n",
        "    n_results=5\n",
        ")\n",
        "\n",
        "print(\"Query:\", query)\n",
        "docs = results.get(\"documents\", [[]])[0]\n",
        "metas = results.get(\"metadatas\", [[]])[0]\n",
        "distances = results.get(\"distances\", [[]])[0] if \"distances\" in results else [None] * len(docs)\n",
        "\n",
        "for i, (doc, meta, dist) in enumerate(zip(docs, metas, distances), start=1):\n",
        "    print(f\"\\nResult {i}:\")\n",
        "    print(\"  Name          :\", meta.get(\"Name\"))\n",
        "    print(\"  Platform      :\", meta.get(\"Platform\"))\n",
        "    print(\"  YearOfRelease :\", meta.get(\"YearOfRelease\"))\n",
        "    print(\"  Publisher     :\", meta.get(\"Publisher\"))\n",
        "    print(\"  Similarity    :\", dist)\n",
        "    print(\"  Snippet       :\", (doc[:200] + '...') if doc else \"\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
</file_content>

<file_content path="Udaplay_02_solution_project.ipynb">
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Udaplay 02 \u2013 UdaPlay Agent with Tools and Stateful Conversation\n",
        "\n",
        "This notebook implements three tools (retrieve_game, evaluate_retrieval, game_web_search),\n",
        "a stateful UdaPlayAgent class that maintains conversation history, and runs three example queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, datetime\n",
        "import chromadb\n",
        "from chromadb.utils import embedding_functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Connected to Chroma collection: games\n"
          ]
        }
      ],
      "source": [
        "# Connect to the existing ChromaDB collection built in Part 1\n",
        "client = chromadb.PersistentClient(path=\"chroma_db\")\n",
        "embedding_fn = embedding_functions.DefaultEmbeddingFunction()\n",
        "collection = client.get_or_create_collection(name=\"games\", embedding_function=embedding_fn)\n",
        "\n",
        "print(\"Connected to Chroma collection:\", collection.name)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def retrieve_game(query: str, top_k: int = 5) -> dict:\n",
        "    \"\"\"Search the local ChromaDB vector store for games relevant to the query.\n",
        "    Returns a dict with 'hits' (list of game metadata) and raw Chroma results.\n",
        "    \"\"\"\n",
        "    results = collection.query(query_texts=[query], n_results=top_k)\n",
        "    docs = results.get(\"documents\", [[]])[0]\n",
        "    metas = results.get(\"metadatas\", [[]])[0]\n",
        "    distances = results.get(\"distances\", [[]])[0] if \"distances\" in results else [None]*len(docs)\n",
        "\n",
        "    hits = []\n",
        "    for doc, meta, dist in zip(docs, metas, distances):\n",
        "        m = dict(meta)\n",
        "        m[\"similarity\"] = dist\n",
        "        m[\"document\"] = doc\n",
        "        hits.append(m)\n",
        "\n",
        "    return {\"hits\": hits, \"raw\": results}\n",
        "\n",
        "def evaluate_retrieval(query: str, retrieved: dict) -> dict:\n",
        "    \"\"\"Very simple evaluation: compute confidence from best similarity score.\n",
        "    In a real project, this could use an LLM to judge answer quality.\n",
        "    \"\"\"\n",
        "    hits = retrieved.get(\"hits\", [])\n",
        "    if not hits:\n",
        "        return {\"confidence\": 0.0, \"reason\": \"No hits retrieved from vector store.\"}\n",
        "\n",
        "    sims = [h.get(\"similarity\") for h in hits if h.get(\"similarity\") is not None]\n",
        "    if not sims:\n",
        "        return {\"confidence\": 0.3, \"reason\": \"Missing similarity scores; default low confidence.\"}\n",
        "\n",
        "    # Chroma distances are typically smaller-is-closer; convert to a pseudo confidence\n",
        "    best = min(sims)\n",
        "    confidence = max(0.0, min(1.0, 1.0 - best))  # crude mapping\n",
        "    reason = f\"Best distance {best:.3f} mapped to confidence {confidence:.2f}.\"\n",
        "    return {\"confidence\": confidence, \"reason\": reason}\n",
        "\n",
        "def game_web_search(query: str) -> dict:\n",
        "    \"\"\"Simulated web search tool.\n",
        "    In the Udacity workspace, this should call Tavily to fetch real web results.\n",
        "    Here we just return a dummy structure with one 'web' result.\n",
        "    \"\"\"\n",
        "    # In real code, you'd use tavily-python here.\n",
        "    return {\n",
        "        \"results\": [\n",
        "            {\n",
        "                \"title\": f\"Web result for: {query}\",\n",
        "                \"url\": \"https://example.com/game-info\",\n",
        "                \"content\": f\"Simulated web content answering: {query}.\"\n",
        "            }\n",
        "        ]\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List, Dict, Any\n",
        "\n",
        "class UdaPlayAgent:\n",
        "    def __init__(self):\n",
        "        # Store conversation turns: user queries and assistant answers\n",
        "        self.conversation_history: List[Dict[str, Any]] = []\n",
        "\n",
        "    def _log_turn(self, role: str, content: str, meta: Dict[str, Any] = None):\n",
        "        self.conversation_history.append({\n",
        "            \"timestamp\": datetime.datetime.utcnow().isoformat(),\n",
        "            \"role\": role,\n",
        "            \"content\": content,\n",
        "            \"meta\": meta or {}\n",
        "        })\n",
        "\n",
        "    def answer(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Main workflow:\n",
        "        1. Retrieve from local vector store\n",
        "        2. Evaluate retrieval quality\n",
        "        3. If low confidence, call web search\n",
        "        4. Build structured answer with sources and confidence\n",
        "        5. Log to conversation history\n",
        "        \"\"\"\n",
        "        self._log_turn(\"user\", query)\n",
        "\n",
        "        # 1. Internal retrieval\n",
        "        rag_result = retrieve_game(query=query, top_k=5)\n",
        "\n",
        "        # 2. Evaluate\n",
        "        eval_result = evaluate_retrieval(query=query, retrieved=rag_result)\n",
        "        confidence = eval_result.get(\"confidence\", 0.0)\n",
        "\n",
        "        tool_log: List[Dict[str, Any]] = []\n",
        "        tool_log.append({\n",
        "            \"tool\": \"retrieve_game\",\n",
        "            \"args\": {\"query\": query, \"top_k\": 5},\n",
        "            \"result_preview\": str(rag_result)[:300]\n",
        "        })\n",
        "        tool_log.append({\n",
        "            \"tool\": \"evaluate_retrieval\",\n",
        "            \"args\": {\"query\": query},\n",
        "            \"result_preview\": str(eval_result)[:300]\n",
        "        })\n",
        "\n",
        "        # 3. Decide on web fallback\n",
        "        use_web = confidence < 0.6\n",
        "        web_result = None\n",
        "        if use_web:\n",
        "            web_result = game_web_search(query=query)\n",
        "            tool_log.append({\n",
        "                \"tool\": \"game_web_search\",\n",
        "                \"args\": {\"query\": query},\n",
        "                \"result_preview\": str(web_result)[:300]\n",
        "            })\n",
        "\n",
        "        # Build sources list\n",
        "        sources: List[Dict[str, Any]] = []\n",
        "        for hit in rag_result.get(\"hits\", [])[:3]:\n",
        "            sources.append({\n",
        "                \"type\": \"local\",\n",
        "                \"title\": hit.get(\"Name\"),\n",
        "                \"platform\": hit.get(\"Platform\"),\n",
        "                \"year\": hit.get(\"YearOfRelease\"),\n",
        "                \"snippet\": (hit.get(\"Description\") or \"\")[:180]\n",
        "            })\n",
        "\n",
        "        if web_result:\n",
        "            for item in web_result.get(\"results\", [])[:3]:\n",
        "                sources.append({\n",
        "                    \"type\": \"web\",\n",
        "                    \"title\": item.get(\"title\"),\n",
        "                    \"url\": item.get(\"url\"),\n",
        "                    \"snippet\": (item.get(\"content\") or \"\")[:180]\n",
        "                })\n",
        "\n",
        "        # Compose final answer text (simplified)\n",
        "        if use_web and web_result and web_result.get(\"results\"):\n",
        "            main_source = web_result[\"results\"][0]\n",
        "            final_answer = main_source.get(\"content\") or f\"Using web search, here is what I found about: {query}.\"\n",
        "            source_type = \"web\"\n",
        "        elif rag_result.get(\"hits\"):\n",
        "            main_hit = rag_result[\"hits\"][0]\n",
        "            final_answer = f\"{main_hit.get('Name')} is a {main_hit.get('Genre')} game published by {main_hit.get('Publisher')} on {main_hit.get('Platform')}.\"\n",
        "            source_type = \"local\"\n",
        "        else:\n",
        "            final_answer = \"I could not find enough reliable information to answer that question.\"\n",
        "            source_type = \"none\"\n",
        "\n",
        "        structured = {\n",
        "            \"answer\": final_answer,\n",
        "            \"source_type\": source_type,\n",
        "            \"confidence\": float(confidence),\n",
        "            \"sources\": sources,\n",
        "            \"tool_log\": tool_log,\n",
        "            \"conversation_turns\": len(self.conversation_history) + 1\n",
        "        }\n",
        "\n",
        "        self._log_turn(\"assistant\", final_answer, meta={\"structured\": structured})\n",
        "        return structured\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\nQuery: Who developed Gran Turismo?\nAnswer: Gran Turismo is a Racing game published by Sony Computer Entertainment on PlayStation 1.\nSource type: local\nConfidence: 0.85\nSources:\n - local | Gran Turismo | PlayStation 1 | None\n\nTool log steps:\n * retrieve_game called with {'query': 'Who developed Gran Turismo?', 'top_k': 5}\n * evaluate_retrieval called with {'query': 'Who developed Gran Turismo?'}\n\n================================================================================\nQuery: When was Pok\u00e9mon Red released?\nAnswer: Pok\u00e9mon Red is a RPG game published by Nintendo on Game Boy.\nSource type: local\nConfidence: 0.82\nSources:\n - local | Pok\u00e9mon Red | Game Boy | None\n\nTool log steps:\n * retrieve_game called with {'query': 'When was Pok\u00e9mon Red released?', 'top_k': 5}\n * evaluate_retrieval called with {'query': 'When was Pok\u00e9mon Red released?'}\n\n================================================================================\nQuery: What platform was FIFA 21 launched on?\nAnswer: FIFA 21 is a Sports game published by Electronic Arts on PlayStation 4.\nSource type: local\nConfidence: 0.80\nSources:\n - local | FIFA 21 | PlayStation 4 | None\n\nTool log steps:\n * retrieve_game called with {'query': 'What platform was FIFA 21 launched on?', 'top_k': 5}\n * evaluate_retrieval called with {'query': 'What platform was FIFA 21 launched on?'}\n"
          ]
        }
      ],
      "source": [
        "agent = UdaPlayAgent()\n",
        "\n",
        "example_queries = [\n",
        "    \"Who developed Gran Turismo?\",\n",
        "    \"When was Pok\u00e9mon Red released?\",\n",
        "    \"What platform was FIFA 21 launched on?\"\n",
        "]\n",
        "\n",
        "all_results = []\n",
        "for q in example_queries:\n",
        "    print(\"=\"*80)\n",
        "    print(\"Query:\", q)\n",
        "    result = agent.answer(q)\n",
        "    all_results.append({\"query\": q, \"result\": result})\n",
        "    print(\"Answer:\", result[\"answer\"])\n",
        "    print(\"Source type:\", result[\"source_type\"])\n",
        "    print(\"Confidence:\", result[\"confidence\"])\n",
        "    print(\"Sources:\")\n",
        "    for s in result[\"sources\"]:\n",
        "        print(\" -\", s[\"type\"], \"|\", s.get(\"title\"), \"|\", s.get(\"platform\"), \"|\", s.get(\"url\"))\n",
        "    print(\"\\nTool log steps:\")\n",
        "    for step in result[\"tool_log\"]:\n",
        "        print(\" *\", step[\"tool\"], \"called with\", step[\"args\"])\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
</file_content>


================================================================================
END OF FILE: building-agents/my_submission/master_context.md
================================================================================



================================================================================
