START OF FILE: udaci-model-optimization/project/my_submission/final_notebooks/01_baseline_colab_pro.ipynb
================================================================================

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UdaciSense: Baseline Performance - Google Colab Pro\n",
    "\n",
    "**üöÄ GPU-accelerated training with Google Colab Pro**\n",
    "\n",
    "**Before starting:**\n",
    "1. Enable GPU: Runtime ‚Üí Change runtime type ‚Üí Hardware accelerator ‚Üí GPU (T4)\n",
    "2. Ensure your project is uploaded to Google Drive\n",
    "3. Update the `DRIVE_PROJECT_PATH` below\n",
    "\n",
    "**Expected training time: ~15-20 minutes with T4 GPU** (vs 12 hours on CPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Google Drive Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Mount Google Drive and navigate to project\n",
    "from google.colab import drive\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# UPDATE THIS PATH to where you uploaded your project\n",
    "DRIVE_PROJECT_PATH = '/content/drive/MyDrive/udacity-projects/udaci-model-optimization/project/starter_kit'\n",
    "\n",
    "# Navigate to project directory\n",
    "os.chdir(DRIVE_PROJECT_PATH)\n",
    "print(f\"‚úÖ Changed to directory: {os.getcwd()}\")\n",
    "\n",
    "# Add project root to Python path for imports\n",
    "project_root = os.path.abspath('../..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "print(f\"‚úÖ Added to Python path: {project_root}\")\n",
    "\n",
    "# Verify project structure\n",
    "required_dirs = ['src', 'notebooks', 'models']\n",
    "missing_dirs = []\n",
    "for d in required_dirs:\n",
    "    if os.path.exists(d):\n",
    "        print(f\"‚úÖ Found: {d}/\")\n",
    "    else:\n",
    "        missing_dirs.append(d)\n",
    "        print(f\"‚ùå Missing: {d}/\")\n",
    "\n",
    "if missing_dirs:\n",
    "    print(f\"\\n‚ö†Ô∏è Please upload these directories to your Google Drive: {missing_dirs}\")\n",
    "else:\n",
    "    print(\"\\nüéâ All required directories found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Install requirements in Colab\n",
    "!pip install -q torch>=2.0.0 torchvision>=0.15.0 \n",
    "!pip install -q matplotlib seaborn pandas scikit-learn pillow tqdm plotly\n",
    "!pip install -q thop  # For FLOPs calculation\n",
    "\n",
    "print(\"‚úÖ All packages installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify GPU Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Check GPU availability\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    gpu_name = torch.cuda.get_device_name(0)\n",
    "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"üöÄ GPU Available: {gpu_name}\")\n",
    "    print(f\"üíæ GPU Memory: {gpu_memory:.1f} GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    print(\"‚ö†Ô∏è No GPU found. Please enable GPU in Runtime ‚Üí Change runtime type\")\n",
    "\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Import Project Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Import all required libraries\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "# Import custom modules\n",
    "try:\n",
    "    from src.utils import MAX_ALLOWED_ACCURACY_DROP, TARGET_INFERENCE_SPEEDUP, TARGET_MODEL_COMPRESSION\n",
    "    from src.utils.data_loader import get_household_loaders, get_input_size, print_dataloader_stats, visualize_batch\n",
    "    from src.utils.model import MobileNetV3_Household, load_model, print_model_summary, train_model\n",
    "    from src.utils.evaluation import calculate_confusion_matrix, evaluate_model_metrics\n",
    "    from src.utils.visualization import plot_confusion_matrix, plot_training_history, plot_weight_distribution\n",
    "    print(\"‚úÖ All custom modules imported successfully!\")\n",
    "    \n",
    "    # Print optimization targets\n",
    "    print(f\"\\nüéØ Optimization Targets:\")\n",
    "    print(f\"   Max accuracy drop: {MAX_ALLOWED_ACCURACY_DROP*100}%\")\n",
    "    print(f\"   Target speedup: {TARGET_INFERENCE_SPEEDUP*100}%\")\n",
    "    print(f\"   Target compression: {TARGET_MODEL_COMPRESSION*100}%\")\n",
    "    \n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Import error: {e}\")\n",
    "    print(\"\\nüí° Troubleshooting:\")\n",
    "    print(\"   1. Check that your project is uploaded to Google Drive\")\n",
    "    print(\"   2. Update DRIVE_PROJECT_PATH in Cell 1\")\n",
    "    print(\"   3. Ensure src/ directory structure is preserved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Set random seed for reproducibility\n",
    "def set_deterministic_mode(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    \n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = seed + worker_id\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "    return seed_worker\n",
    "\n",
    "seed_worker = set_deterministic_mode(42)\n",
    "g = torch.Generator()\n",
    "g.manual_seed(42)\n",
    "\n",
    "print(\"‚úÖ Reproducibility mode set (seed=42)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Setup Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Create directories for results\n",
    "model_type = \"baseline_mobilenet_colab\"\n",
    "models_dir = f\"models/{model_type}\"\n",
    "models_ckp_dir = f\"{models_dir}/checkpoints\"\n",
    "results_dir = f\"results/{model_type}\"\n",
    "\n",
    "# Create directories\n",
    "os.makedirs(models_ckp_dir, exist_ok=True)\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ Created directories:\")\n",
    "print(f\"   Models: {models_dir}\")\n",
    "print(f\"   Checkpoints: {models_ckp_dir}\")\n",
    "print(f\"   Results: {results_dir}\")\n",
    "\n",
    "# These will be synced back to your Google Drive automatically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Load Dataset (GPU Optimized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Load household objects dataset\n",
    "# Larger batch size for GPU efficiency\n",
    "GPU_BATCH_SIZE = 256 if torch.cuda.is_available() else 128\n",
    "NUM_WORKERS = 2  # Colab works best with 2 workers\n",
    "\n",
    "print(f\"üîÑ Loading dataset (batch_size={GPU_BATCH_SIZE}, num_workers={NUM_WORKERS})...\")\n",
    "\n",
    "train_loader, test_loader = get_household_loaders(\n",
    "    image_size=\"CIFAR\", \n",
    "    batch_size=GPU_BATCH_SIZE, \n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "# Get class information\n",
    "class_names = train_loader.dataset.classes\n",
    "print(f\"\\nüìä Dataset loaded with {len(class_names)} classes:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    print(f\"   {i}: {name}\")\n",
    "\n",
    "# Print dataset stats\n",
    "for name, loader in [('Train', train_loader), ('Test', test_loader)]:\n",
    "    print(f\"\\n{name} set statistics:\")\n",
    "    print_dataloader_stats(loader, name.lower())\n",
    "\n",
    "# Show sample images\n",
    "print(\"\\nüñºÔ∏è Sample images from training set:\")\n",
    "visualize_batch(train_loader, num_images=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Initialize and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Initialize MobileNetV3 model\n",
    "print(\"üèóÔ∏è Initializing MobileNetV3 model...\")\n",
    "model = MobileNetV3_Household().to(device)\n",
    "print_model_summary(model)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"\\nüíæ GPU Memory after model loading: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Configure training (optimized for GPU)\n",
    "# Fewer epochs since GPU trains more efficiently\n",
    "num_epochs = 30\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Scale learning rate with batch size\n",
    "base_lr = 0.001 * (GPU_BATCH_SIZE / 128)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=base_lr,\n",
    "    weight_decay=1e-4,\n",
    "    betas=(0.9, 0.999)\n",
    ")\n",
    "\n",
    "# OneCycleLR for faster convergence\n",
    "max_lr = 0.005 * (GPU_BATCH_SIZE / 128)\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=max_lr,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    epochs=num_epochs,\n",
    "    pct_start=0.3,\n",
    "    div_factor=25,\n",
    "    final_div_factor=1000\n",
    ")\n",
    "\n",
    "training_config = {\n",
    "    'num_epochs': num_epochs,\n",
    "    'criterion': criterion,\n",
    "    'optimizer': optimizer,\n",
    "    'scheduler': scheduler,\n",
    "    'patience': 7,\n",
    "    'device': device\n",
    "}\n",
    "\n",
    "print(f\"‚öôÔ∏è Training Configuration:\")\n",
    "print(f\"   Epochs: {num_epochs}\")\n",
    "print(f\"   Base LR: {base_lr:.6f}\")\n",
    "print(f\"   Max LR: {max_lr:.6f}\")\n",
    "print(f\"   Batch Size: {GPU_BATCH_SIZE}\")\n",
    "print(f\"   Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Train the model\n",
    "checkpoint_path = f\"{models_ckp_dir}/model.pth\"\n",
    "\n",
    "print(f\"üöÄ Starting training on {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   Expected time: ~15-20 minutes on T4 GPU\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è Running on CPU - this will take much longer!\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Train the model\n",
    "training_stats, best_accuracy, best_epoch = train_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    training_config,\n",
    "    checkpoint_path=checkpoint_path,\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\n‚úÖ Training completed in {training_time/60:.1f} minutes!\")\n",
    "print(f\"üèÜ Best accuracy: {best_accuracy:.4f} at epoch {best_epoch}\")\n",
    "\n",
    "# Save training stats\n",
    "with open(f\"{results_dir}/training_stats.json\", 'w') as f:\n",
    "    json.dump(training_stats, f, indent=4)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    cpu_time_hours = 12  # Estimated CPU time\n",
    "    speedup = (cpu_time_hours * 60) / (training_time / 60)\n",
    "    print(f\"\\nüöÄ GPU Speedup: ~{speedup:.1f}x faster than CPU training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: Comprehensive model evaluation\n",
    "print(\"üîç Evaluating model performance...\")\n",
    "\n",
    "# Load best model\n",
    "model = load_model(checkpoint_path, device)\n",
    "\n",
    "# Evaluation parameters\n",
    "n_classes = len(class_names)\n",
    "input_size = get_input_size(\"CIFAR\")\n",
    "\n",
    "# Calculate metrics\n",
    "start_eval = time.time()\n",
    "baseline_metrics = evaluate_model_metrics(\n",
    "    model, test_loader, device, n_classes, class_names, input_size,\n",
    "    save_path=f\"{results_dir}/metrics.json\"\n",
    ")\n",
    "eval_time = time.time() - start_eval\n",
    "\n",
    "print(f\"‚úÖ Evaluation completed in {eval_time:.1f} seconds\")\n",
    "\n",
    "# Display key metrics\n",
    "print(f\"\\nüìä BASELINE PERFORMANCE:\")\n",
    "print(f\"   üéØ Top-1 Accuracy: {baseline_metrics['accuracy']['top1_acc']:.2f}%\")\n",
    "print(f\"   üéØ Top-5 Accuracy: {baseline_metrics['accuracy']['top5_acc']:.2f}%\")\n",
    "print(f\"   üìè Model Size: {baseline_metrics['size']['model_size_mb']:.2f} MB\")\n",
    "print(f\"   üî¢ Parameters: {baseline_metrics['size']['num_params']:,}\")\n",
    "print(f\"   ‚è±Ô∏è CPU Inference: {baseline_metrics['timing']['cpu']['avg_time_ms']:.2f} ms\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ‚ö° GPU Inference: {baseline_metrics['timing']['cuda']['avg_time_ms']:.2f} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: Generate visualizations\n",
    "print(\"üìà Generating visualizations...\")\n",
    "\n",
    "# Confusion matrix\n",
    "confusion_matrix = calculate_confusion_matrix(model, test_loader, device, n_classes)\n",
    "plot_confusion_matrix(confusion_matrix, class_names, f\"{results_dir}/confusion_matrix.png\")\n",
    "\n",
    "# Training history\n",
    "plot_training_history(training_stats, f\"{results_dir}/training_history.png\")\n",
    "\n",
    "# Weight distribution\n",
    "plot_weight_distribution(model, output_path=f\"{results_dir}/weight_distribution.png\")\n",
    "\n",
    "print(\"‚úÖ All visualizations saved to results directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Calculate Optimization Targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: Calculate and display optimization targets\n",
    "# Calculate targets based on CTO requirements\n",
    "target_model_size = baseline_metrics['size']['model_size_mb'] * (1 - TARGET_MODEL_COMPRESSION)\n",
    "target_cpu_time = baseline_metrics['timing']['cpu']['avg_time_ms'] * (1 - TARGET_INFERENCE_SPEEDUP)\n",
    "min_accuracy = baseline_metrics['accuracy']['top1_acc'] * (1 - MAX_ALLOWED_ACCURACY_DROP)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ OPTIMIZATION TARGETS FOR COMPRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nüìè SIZE TARGET:\")\n",
    "print(f\"   Current: {baseline_metrics['size']['model_size_mb']:.2f} MB\")\n",
    "print(f\"   Target:  {target_model_size:.2f} MB\")\n",
    "print(f\"   Reduction: {TARGET_MODEL_COMPRESSION*100}%\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è SPEED TARGET (CPU):\")\n",
    "print(f\"   Current: {baseline_metrics['timing']['cpu']['avg_time_ms']:.2f} ms\")\n",
    "print(f\"   Target:  {target_cpu_time:.2f} ms\")\n",
    "print(f\"   Speedup: {TARGET_INFERENCE_SPEEDUP*100}%\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    target_gpu_time = baseline_metrics['timing']['cuda']['avg_time_ms'] * (1 - TARGET_INFERENCE_SPEEDUP)\n",
    "    print(f\"\\n‚ö° SPEED TARGET (GPU):\")\n",
    "    print(f\"   Current: {baseline_metrics['timing']['cuda']['avg_time_ms']:.2f} ms\")\n",
    "    print(f\"   Target:  {target_gpu_time:.2f} ms\")\n",
    "    print(f\"   Speedup: {TARGET_INFERENCE_SPEEDUP*100}%\")\n",
    "\n",
    "print(f\"\\nüéØ ACCURACY TARGET:\")\n",
    "print(f\"   Current: {baseline_metrics['accuracy']['top1_acc']:.2f}%\")\n",
    "print(f\"   Minimum: {min_accuracy:.2f}%\")\n",
    "print(f\"   Max drop: {MAX_ALLOWED_ACCURACY_DROP*100}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "# Save targets for next notebook\n",
    "targets = {\n",
    "    'baseline_accuracy': baseline_metrics['accuracy']['top1_acc'],\n",
    "    'baseline_size_mb': baseline_metrics['size']['model_size_mb'],\n",
    "    'baseline_cpu_ms': baseline_metrics['timing']['cpu']['avg_time_ms'],\n",
    "    'target_size_mb': target_model_size,\n",
    "    'target_cpu_ms': target_cpu_time,\n",
    "    'min_accuracy': min_accuracy,\n",
    "    'training_time_minutes': training_time / 60\n",
    "}\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    targets['baseline_gpu_ms'] = baseline_metrics['timing']['cuda']['avg_time_ms']\n",
    "    targets['target_gpu_ms'] = target_gpu_time\n",
    "\n",
    "with open(f\"{results_dir}/optimization_targets.json\", 'w') as f:\n",
    "    json.dump(targets, f, indent=4)\n",
    "\n",
    "print(f\"üíæ Results saved to: {results_dir}/\")\n",
    "print(f\"üìÅ All files are automatically synced to your Google Drive!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéâ Training Complete!\n",
    "\n",
    "**Your baseline model has been successfully trained with GPU acceleration!**\n",
    "\n",
    "### üìä What You Achieved:\n",
    "- ‚úÖ **GPU Training**: ~15-20 minutes vs 12 hours on CPU\n",
    "- ‚úÖ **Baseline Established**: Ready for optimization\n",
    "- ‚úÖ **All Results Saved**: Models, metrics, and visualizations in Google Drive\n",
    "\n",
    "### üöÄ Next Steps:\n",
    "1. **Review your results** in the `results/baseline_mobilenet_colab/` folder\n",
    "2. **Implement compression techniques** in the next notebook\n",
    "3. **Meet the CTO targets**: 70% size reduction, 60% speed improvement\n",
    "\n",
    "### üíæ Your Files (Auto-synced to Drive):\n",
    "- `models/baseline_mobilenet_colab/checkpoints/model.pth` - Best model\n",
    "- `results/baseline_mobilenet_colab/metrics.json` - Performance metrics\n",
    "- `results/baseline_mobilenet_colab/*.png` - Visualizations\n",
    "\n",
    "**Ready to optimize! üéØ**"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "gpuType": "T4"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  },
  "accelerator": "GPU"
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

================================================================================
END OF FILE: udaci-model-optimization/project/my_submission/final_notebooks/01_baseline_colab_pro.ipynb
================================================================================



================================================================================
